<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zisedeqing</title>
  
  
  <link href="https://zisedeqing.github.io/atom.xml" rel="self"/>
  
  <link href="https://zisedeqing.github.io/"/>
  <updated>2021-01-21T08:25:23.165Z</updated>
  <id>https://zisedeqing.github.io/</id>
  
  <author>
    <name>zisedeqing</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>收藏的技术网页</title>
    <link href="https://zisedeqing.github.io/2021/01/20/%E6%94%B6%E8%97%8F%E7%9A%84%E7%BD%91%E9%A1%B5/"/>
    <id>https://zisedeqing.github.io/2021/01/20/%E6%94%B6%E8%97%8F%E7%9A%84%E7%BD%91%E9%A1%B5/</id>
    <published>2021-01-20T12:56:21.000Z</published>
    <updated>2021-01-21T08:25:23.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一致性hash算法"><a href="#一致性hash算法" class="headerlink" title="一致性hash算法"></a><a href="https://www.cnblogs.com/lpfuture/p/5796398.html">一致性hash算法</a></h2><h2 id="RAFT"><a href="#RAFT" class="headerlink" title="RAFT"></a>RAFT</h2><ol><li><a href="https://blog.csdn.net/shangsongwww/article/details/90287565">Raft算法原理</a></li><li>两篇raft论文的翻译：<br><a href="https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md">寻找一种易于理解的一致性算法（扩展版）</a><br><a href="https://www.cnblogs.com/linbingdong/p/6442673.html">分布式一致性算法：Raft 算法（论文翻译）</a></li></ol><h2 id="Snowflake"><a href="#Snowflake" class="headerlink" title="Snowflake"></a>Snowflake</h2><p>   <a href="https://zhuanlan.zhihu.com/p/56745552?utm_source=wechat_session">读后感之《The Snowflake Elastic Data Warehouse》</a></p><h2 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h2><p>   <a href="https://cn.greenplum.org/%e7%bb%88%e4%ba%8e%e6%8a%8a%e5%88%86%e5%b8%83%e5%bc%8f%e4%ba%8b%e5%8a%a1%e8%ae%b2%e6%98%8e%e7%99%bd%e4%ba%86%ef%bc%81/">greenplum 分布式事务</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一致性hash算法&quot;&gt;&lt;a href=&quot;#一致性hash算法&quot; class=&quot;headerlink&quot; title=&quot;一致性hash算法&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.cnblogs.com/lpfuture/p/5796398.html&quot;&gt;一</summary>
      
    
    
    
    <category term="收藏" scheme="https://zisedeqing.github.io/categories/%E6%94%B6%E8%97%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>pg 大会--GaussDB 技术探索</title>
    <link href="https://zisedeqing.github.io/2021/01/15/pg-%E5%A4%A7%E4%BC%9A-gauss-db-%E6%8A%80%E6%9C%AF%E6%8E%A2%E7%B4%A2/"/>
    <id>https://zisedeqing.github.io/2021/01/15/pg-%E5%A4%A7%E4%BC%9A-gauss-db-%E6%8A%80%E6%9C%AF%E6%8E%A2%E7%B4%A2/</id>
    <published>2021-01-15T07:47:51.000Z</published>
    <updated>2021-01-15T08:10:31.387Z</updated>
    
    <content type="html"><![CDATA[<p>产品是GaussDB(for postgreSQL)</p><a id="more"></a><h2 id="去掉full-page-write"><a href="#去掉full-page-write" class="headerlink" title="去掉full page write"></a>去掉full page write</h2><pre><code>基于共享存储，存储层保证8Kpage的原子写</code></pre><h2 id="预读"><a href="#预读" class="headerlink" title="预读"></a>预读</h2><pre><code>pg基于文件系统的预读，共享存储没有，内核实现预读功能</code></pre><h2 id="tuple-hints"><a href="#tuple-hints" class="headerlink" title="tuple hints"></a>tuple hints</h2><pre><code>pg的tuple hints不记录xlog，复制到standby后，会影响standby的性能添加tuple hints的wal log，在页面淘汰时，批量记录。</code></pre><h2 id="并行redo"><a href="#并行redo" class="headerlink" title="并行redo"></a>并行redo</h2><p><img src="/2021/01/15/pg-%E5%A4%A7%E4%BC%9A-gauss-db-%E6%8A%80%E6%9C%AF%E6%8E%A2%E7%B4%A2/xlog-redo.png" alt="xlog-redo"></p><h2 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h2><p><img src="/2021/01/15/pg-%E5%A4%A7%E4%BC%9A-gauss-db-%E6%8A%80%E6%9C%AF%E6%8E%A2%E7%B4%A2/gaussdb-pg.png" alt="整体架构图"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;产品是GaussDB(for postgreSQL)&lt;/p&gt;</summary>
    
    
    
    <category term="pg 大会" scheme="https://zisedeqing.github.io/categories/pg-%E5%A4%A7%E4%BC%9A/"/>
    
    <category term="GaussDB" scheme="https://zisedeqing.github.io/categories/pg-%E5%A4%A7%E4%BC%9A/GaussDB/"/>
    
    
  </entry>
  
  <entry>
    <title>阿里云RDS-数据库内核月报</title>
    <link href="https://zisedeqing.github.io/2021/01/11/%E9%98%BF%E9%87%8C%E4%BA%91RDS-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E6%A0%B8%E6%9C%88%E6%8A%A5/"/>
    <id>https://zisedeqing.github.io/2021/01/11/%E9%98%BF%E9%87%8C%E4%BA%91RDS-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E6%A0%B8%E6%9C%88%E6%8A%A5/</id>
    <published>2021-01-11T09:25:23.000Z</published>
    <updated>2021-01-11T09:27:06.019Z</updated>
    
    <content type="html"><![CDATA[<p>网站: <a href="https://www.bookstack.cn/books/aliyun-rds-core">https://www.bookstack.cn/books/aliyun-rds-core</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网站: &lt;a href=&quot;https://www.bookstack.cn/books/aliyun-rds-core&quot;&gt;https://www.bookstack.cn/books/aliyun-rds-core&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="收藏" scheme="https://zisedeqing.github.io/categories/%E6%94%B6%E8%97%8F/"/>
    
    
    <category term="阿里云" scheme="https://zisedeqing.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
    <category term="RDS" scheme="https://zisedeqing.github.io/tags/RDS/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum 写入磁盘IO异常高的问题分析</title>
    <link href="https://zisedeqing.github.io/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/"/>
    <id>https://zisedeqing.github.io/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</id>
    <published>2020-12-29T07:59:47.000Z</published>
    <updated>2020-12-29T12:03:04.353Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gp7-写入高问题分析"><a href="#gp7-写入高问题分析" class="headerlink" title="gp7 写入高问题分析"></a>gp7 写入高问题分析</h1><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>对比gp7单master 与pg9.6 测试发现一个问题：gp7的磁盘写入量是pg9.6的好几倍，具体表现为：iostat时发现gp7的每秒写入量是pg9.6的好几倍，还有就是查看gplog发现在10分钟的测试期间gp做了2次由于xlog触发的checkpoint，而pg未触发。</p><a id="more"></a><p><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/io-write.png" alt="io write"><br><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/io-utils.png" alt="io utils"></p><p>根据测试结果，gp7的tpmc比pg9.6要小3倍，按理说，gp7的io写入量应该比pg9.6小才对，但是从系统监控看却不是这样的，需要具体分析。</p><p>gpdb 编译参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;home&#x2F;xxx&#x2F;gpdb-install --with-openssl --with-ldap --with-libxml --with-gssapi --enable-debug --disable-orca --disable-oss-ext --with-blocksize&#x3D;8 --with-wal-blocksize&#x3D;8 CFLAGS&#x3D;&quot;-O2 -DIMPLEMENT_ASYNC_COMMIT&quot;</span><br></pre></td></tr></table></figure><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><h2 id="问题1：为何gp7的backend会有比较大量的io？"><a href="#问题1：为何gp7的backend会有比较大量的io？" class="headerlink" title="问题1：为何gp7的backend会有比较大量的io？"></a>问题1：为何gp7的backend会有比较大量的io？</h2><p>使用strace和lsof监控 gp7和pg9.6的数据写入信息，并比较，发现pg9.6时数据基本都wal writer 进程写入的，而backend进程只有少量数据写入，但是gp7的wal writer和backend都会有大量数据写入操作，具体分析：<br>测试命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">strace -tt -T -f -e trace&#x3D;write -p 89761</span><br><span class="line">lsof -p pid1,pid2</span><br></pre></td></tr></table></figure><p><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/strace.png" alt="strace"></p><p>wal writer:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">postgres  58764 3u      REG     259,2   67108864 170265362 &#x2F;home&#x2F;admin&#x2F;gp_data&#x2F;master&#x2F;gpseg-1&#x2F;pg_xlog&#x2F;00000001000000820000000D</span><br></pre></td></tr></table></figure><p><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/strace2.png" alt="strace"><br>在运行的时候wal writer都是针对pg_xlog/00000001000000820000000D的写入</p><p>backend：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">postgres 198841 126u      REG     259,2   67108864 170265362 &#x2F;home&#x2F;admin&#x2F;gp_data&#x2F;master&#x2F;gpseg-1&#x2F;pg_xlog&#x2F;00000001000000820000000D</span><br></pre></td></tr></table></figure><p>backend的write操作有2种，一是针对数据文件的写入，量比较少，是有一种是针对xlog的写入，量很大，是数据文件写入的7倍左右：<br><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/strace-backend.png" alt="strace backend"></p><p><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/strace-backend2.png" alt="strace backend"></p><p>分析代码发现2个问题：</p><ol><li>gp添加了IMPLEMENT_ASYNC_COMMIT编译宏，默认没有开启异步提交</li><li>gp默认wal block size为32k，pg为8k</li></ol><p>pg9.6：<br><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/pg9.6.png" alt="image.png"></p><p>重启编译gp7，开启异步提交并把wal block size改为8后，gp7 backend的写入与pg9.6一致了，但是gp7的wal 整体写入量还是比pg9.6大，需要进一步分析。</p><h2 id="问题2：为何gp7的wal-log会比pg9-6的大"><a href="#问题2：为何gp7的wal-log会比pg9-6的大" class="headerlink" title="问题2：为何gp7的wal log会比pg9.6的大"></a>问题2：为何gp7的wal log会比pg9.6的大</h2><p>重新编译gp7和pg9.6开启wal_debug功能，并且在运行的时候打开wal_debug功能，分析运行时产生的wal log发现：<br><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/xlog-type.png" alt="xlog type"><br>gp7 比pg9.6多记录了一直类型的xlog：XLOG/FPI_FOR_HINT， 该日志是在tuple的hints发生变化时记录的，分析代码发现2个触发条件：</p><ol><li>controlfile中的Data page checksum version &gt; 0</li><li>guc 参数wal_log_hints开启</li></ol><p>对比gp7和gp9.6 发现 gp7的 Data page checksum version 为1，而pg9.6默认为0。 查看gpinitsystem是的log发现：<br><img src="/2020/12/29/Greenplum-%E5%86%99%E5%85%A5%E7%A3%81%E7%9B%98IO%E5%BC%82%E5%B8%B8%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/initsystem.png" alt="image.png"><br>gp在初始化数据库时默认开启的page check sum功能。</p><p>重新初始化数据库并且关闭page checksum后，gp7 io有所降低，但是依然比pg9.6高很多。对gp7和pg9.6分别执行如下操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">checkpoint;</span><br><span class="line">select pg_switch_xlog();</span><br><span class="line">select pg_current_xlog_insert_location();</span><br><span class="line"></span><br><span class="line">tpcc测试64并发跑200s</span><br><span class="line">select pg_current_xlog_insert_location();</span><br><span class="line"></span><br><span class="line">计算90s产生的日志量，并使用pg_xlogdump工具分析这些日志</span><br></pre></td></tr></table></figure><p>测试结果分别如下：<br>gp7 master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">master_only&#x3D;# select pg_switch_xlog();</span><br><span class="line"> pg_switch_xlog</span><br><span class="line">----------------</span><br><span class="line"> 87&#x2F;DFFEBD08</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">Time: 0.643 ms</span><br><span class="line">master_only&#x3D;# select pg_current_xlog_insert_location();</span><br><span class="line"> pg_current_xlog_insert_location</span><br><span class="line">---------------------------------</span><br><span class="line"> 87&#x2F;E0000028</span><br><span class="line">(1 row)</span><br><span class="line">.&#x2F;tpcc.lua --pgsql-host&#x3D;127.0.0.1 --pgsql-port&#x3D;35432 --pgsql-user&#x3D;benchmarksql --pgsql-db&#x3D;master_only     --threads&#x3D;64 --tables&#x3D;1 --scale&#x3D;1000 --trx_level&#x3D;RC --db-ps-mode&#x3D;auto --db-driver&#x3D;pgsql --time&#x3D;200     --report-interval&#x3D;10 run</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">开始的xlog: 0000000100000087000000E0</span><br><span class="line"></span><br><span class="line">master_only&#x3D;# select pg_current_xlog_insert_location();</span><br><span class="line"> pg_current_xlog_insert_location</span><br><span class="line">---------------------------------</span><br><span class="line"> 95&#x2F;E46502E8</span><br><span class="line"></span><br><span class="line">结束的xlog： 0000000100000095000000E4</span><br><span class="line">xlog量：57414MB</span><br><span class="line">transactions:                        1935328 (9667.02 per sec.)</span><br><span class="line"></span><br><span class="line">$pg_xlogdump -z --stats&#x3D;record 0000000100000087000000E0 0000000100000095000000E4</span><br><span class="line">Type                                           N      (%)          Record size      (%)            FPI count      (%)             FPI size      (%)        Combined size      (%)</span><br><span class="line">----                                           -      ---          -----------      ---            ---------      ---             --------      ---        -------------      ---</span><br><span class="line">XLOG&#x2F;CHECKPOINT_ONLINE                         1 (  0.00)                  116 (  0.00)                    0 (  0.00)                    0 (  0.00)                  116 (  0.00)</span><br><span class="line">Transaction&#x2F;COMMIT                       1757494 (  2.06)             70299760 (  2.63)                    0 (  0.00)                    0 (  0.00)             70299760 (  0.13)</span><br><span class="line">Transaction&#x2F;ABORT                           8422 (  0.01)               336880 (  0.01)                    0 (  0.00)                    0 (  0.00)               336880 (  0.00)</span><br><span class="line">CLOG&#x2F;ZEROPAGE                                 54 (  0.00)                 1512 (  0.00)                    0 (  0.00)                    0 (  0.00)                 1512 (  0.00)</span><br><span class="line">Heap2&#x2F;CLEAN                             11772482 ( 13.79)            376719424 ( 14.12)               986271 ( 13.06)           7200248456 ( 13.49)           7576967880 ( 13.52)</span><br><span class="line">Heap2&#x2F;LOCK_UPDATED                          2290 (  0.00)                73280 (  0.00)                    0 (  0.00)                    0 (  0.00)                73280 (  0.00)</span><br><span class="line">Heap&#x2F;INSERT                             10789086 ( 12.64)            291305322 ( 10.92)                   87 (  0.00)               376976 (  0.00)            291682298 (  0.52)</span><br><span class="line">Heap&#x2F;DELETE                               842732 (  0.99)             26967424 (  1.01)                30278 (  0.40)            223418376 (  0.42)            250385800 (  0.45)</span><br><span class="line">Heap&#x2F;UPDATE                              2289529 (  2.68)             87002102 (  3.26)                   10 (  0.00)                48032 (  0.00)             87050134 (  0.16)</span><br><span class="line">Heap&#x2F;UPDATE_LOCK                         2318276 (  2.72)             74184832 (  2.78)               131288 (  1.74)           1060088220 (  1.99)           1134273052 (  2.02)</span><br><span class="line">Heap&#x2F;HOT_UPDATE                         19560240 ( 22.92)            743289120 ( 27.85)               355980 (  4.71)           2794941056 (  5.24)           3538230176 (  6.31)</span><br><span class="line">Heap&#x2F;LOCK                               10081592 ( 11.81)            322610944 ( 12.09)              4089915 ( 54.15)          31260818368 ( 58.58)          31583429312 ( 56.37)</span><br><span class="line">Heap&#x2F;INSERT+INIT                          129354 (  0.15)              3492558 (  0.13)                    0 (  0.00)                    0 (  0.00)              3492558 (  0.01)</span><br><span class="line">Heap&#x2F;UPDATE+INIT                           28747 (  0.03)              1092386 (  0.04)                    0 (  0.00)                    0 (  0.00)              1092386 (  0.00)</span><br><span class="line">Btree&#x2F;INSERT_LEAF                       25515164 ( 29.90)            663394264 ( 24.86)              1910997 ( 25.30)          10574858940 ( 19.82)          11238253204 ( 20.06)</span><br><span class="line">Btree&#x2F;INSERT_UPPER                        117204 (  0.14)              3047304 (  0.11)                17182 (  0.23)             92099280 (  0.17)             95146584 (  0.17)</span><br><span class="line">Btree&#x2F;SPLIT_L                              23978 (  0.03)               767296 (  0.03)                 6179 (  0.08)             34688400 (  0.07)             35455696 (  0.06)</span><br><span class="line">Btree&#x2F;SPLIT_R                              93451 (  0.11)              2990432 (  0.11)                24384 (  0.32)            111544716 (  0.21)            114535148 (  0.20)</span><br><span class="line">Btree&#x2F;DELETE                               19071 (  0.02)               940340 (  0.04)                 1031 (  0.01)              8269024 (  0.02)              9209364 (  0.02)</span><br><span class="line">                                        --------                      --------                      --------                      --------                      --------</span><br><span class="line">Total                                   85349167                    2668515296 [4.76%]               7553602 [8.85%]           53361399844 [95.24%]          56029915140 [100%]</span><br></pre></td></tr></table></figure><p>pg9.6:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">tpcc&#x3D;# select pg_switch_xlog();</span><br><span class="line"> pg_switch_xlog</span><br><span class="line">----------------</span><br><span class="line"> F3&#x2F;E2242308</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">tpcc&#x3D;# select pg_current_xlog_insert_location();</span><br><span class="line"> pg_current_xlog_insert_location</span><br><span class="line">---------------------------------</span><br><span class="line"> F3&#x2F;E3000028</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">开始的xlog: 00000001000000F3000000E3</span><br><span class="line"></span><br><span class="line">.&#x2F;tpcc.lua --pgsql-host&#x3D;127.0.0.1 --pgsql-port&#x3D;11000 --pgsql-user&#x3D;benchmarksql --pgsql-db&#x3D;tpcc     --threads&#x3D;64 --tables&#x3D;1 --scale&#x3D;100 --trx_level&#x3D;RC --db-ps-mode&#x3D;auto --db-driver&#x3D;pgsql --time&#x3D;200     --report-interval&#x3D;20 run</span><br><span class="line"></span><br><span class="line">tpcc&#x3D;# select pg_current_xlog_insert_location();</span><br><span class="line"> pg_current_xlog_insert_location</span><br><span class="line">---------------------------------</span><br><span class="line"> F9&#x2F;9BC4DB20</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">结束的xlog: 00000001000000F90000009B</span><br><span class="line">写入的xlog量：23436 MB</span><br><span class="line">transactions:                        3711540 (18553.94 per sec.)</span><br><span class="line"></span><br><span class="line">$&#x2F;home&#x2F;shanghao.zxb&#x2F;pg-install-9.6&#x2F;bin&#x2F;pg_xlogdump -z --stats&#x3D;record 00000001000000F3000000E3 00000001000000F90000009B</span><br><span class="line">^@Type                                           N      (%)          Record size      (%)            FPI count      (%)             FPI size      (%)        Combined size      (%)</span><br><span class="line">----                                           -      ---          -----------      ---            ---------      ---             --------      ---        -------------      ---</span><br><span class="line">XLOG&#x2F;CHECKPOINT_ONLINE                         1 (  0.00)                  104 (  0.00)                    0 (  0.00)                    0 (  0.00)                  104 (  0.00)</span><br><span class="line">Transaction&#x2F;COMMIT                       3409769 (  1.99)            109112608 (  2.04)                    0 (  0.00)                    0 (  0.00)            109112608 (  0.65)</span><br><span class="line">Transaction&#x2F;ABORT                          16378 (  0.01)               524096 (  0.01)                    0 (  0.00)                    0 (  0.00)               524096 (  0.00)</span><br><span class="line">CLOG&#x2F;ZEROPAGE                                105 (  0.00)                 2940 (  0.00)                    0 (  0.00)                    0 (  0.00)                 2940 (  0.00)</span><br><span class="line">MultiXact&#x2F;CREATE_ID                          203 (  0.00)                10604 (  0.00)                    0 (  0.00)                    0 (  0.00)                10604 (  0.00)</span><br><span class="line">Heap2&#x2F;CLEAN                             22999350 ( 13.40)            735979200 ( 13.78)               362013 ( 21.09)           2624962084 ( 22.88)           3360941284 ( 19.99)</span><br><span class="line">Heap2&#x2F;LOCK_UPDATED                         68615 (  0.04)              2195680 (  0.04)                    0 (  0.00)                    0 (  0.00)              2195680 (  0.01)</span><br><span class="line">Heap&#x2F;INSERT                             20933838 ( 12.20)            565213626 ( 10.58)                  110 (  0.01)               588420 (  0.01)            565802046 (  3.37)</span><br><span class="line">Heap&#x2F;DELETE                              1623527 (  0.95)             51952864 (  0.97)                 7081 (  0.41)             54214904 (  0.47)            106167768 (  0.63)</span><br><span class="line">Heap&#x2F;UPDATE                              4739123 (  2.76)            180086674 (  3.37)                   36 (  0.00)               221620 (  0.00)            180308294 (  1.07)</span><br><span class="line">Heap&#x2F;UPDATE_LOCK                         4797586 (  2.79)            153522752 (  2.87)               113312 (  6.60)            922353016 (  8.04)           1075875768 (  6.40)</span><br><span class="line">Heap&#x2F;HOT_UPDATE                         37650344 ( 21.93)           1430713072 ( 26.79)                66795 (  3.89)            510473008 (  4.45)           1941186080 ( 11.55)</span><br><span class="line">Heap&#x2F;LOCK                               24518122 ( 14.28)            784579904 ( 14.69)               462274 ( 26.93)           3464743948 ( 30.20)           4249323852 ( 25.28)</span><br><span class="line">Heap&#x2F;INSERT+INIT                          244941 (  0.14)              6613407 (  0.12)                    0 (  0.00)                    0 (  0.00)              6613407 (  0.04)</span><br><span class="line">Heap&#x2F;UPDATE+INIT                           58463 (  0.03)              2221594 (  0.04)                    0 (  0.00)                    0 (  0.00)              2221594 (  0.01)</span><br><span class="line">Btree&#x2F;INSERT_LEAF                       50086170 ( 29.18)           1302240420 ( 24.38)               694187 ( 40.44)           3834843816 ( 33.43)           5137084236 ( 30.56)</span><br><span class="line">Btree&#x2F;INSERT_UPPER                        234535 (  0.14)              6097910 (  0.11)                 3704 (  0.22)             20528312 (  0.18)             26626222 (  0.16)</span><br><span class="line">Btree&#x2F;SPLIT_L                              52540 (  0.03)              1681280 (  0.03)                 1938 (  0.11)             11472524 (  0.10)             13153804 (  0.08)</span><br><span class="line">Btree&#x2F;SPLIT_R                             182884 (  0.11)              5852288 (  0.11)                 4976 (  0.29)             24856244 (  0.22)             30708532 (  0.18)</span><br><span class="line">Btree&#x2F;DELETE                               40755 (  0.02)              1990646 (  0.04)                  276 (  0.02)              2141720 (  0.02)              4132366 (  0.02)</span><br><span class="line">                                        --------                      --------                      --------                      --------                      --------</span><br><span class="line">Total                                  171657249                    5340591669 [31.77%]              1716702 [1.00%]           11471399616 [68.23%]          16811991285 [100%]</span><br></pre></td></tr></table></figure><p>从上面的测试结果看：</p><ol><li>gp7 的xlog 量是pg9.6一倍多，但是tpc只有其一半</li></ol><ol start="2"><li>gp7中FPI 中的比重太多，占了总wal log的95%</li></ol><p>FPI: full page image</p><ol start="3"><li>在FPI中，Heap/LOCK类型 gp7占的比重明显过高</li></ol><p>修改pg_xlogdump 添加fpi count的统计，统计带有fpi的xlog record的个数，根据统计结果看出：</p><ol><li>xlog 量的增加是由于带有fpi的xlog record增加导致的</li><li>heap/lock 类型带有fpi的xlog占比如下：</li></ol><p>gp7 4089915/10081592 = 40.8%<br>pg9.6 462274/24518122 = 1.8%</p><p>进一步分析xlog，统计200s内gp7与pg9.6中修改的page个数如下:<br>gp7 总的page个数: 8292522<br>pg9.6总的page个数: 1813842<br>gp 7在tpmc是pg9.6一半的情况下，修改的page个数却是pg9.6的7倍。这里统计的修改的页面个数只计算了相同page的第一次修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 编译：开启异步提交，wal block size 8</span><br><span class="line">2. 关闭 page checksum</span><br><span class="line">3. 未尝试的操作：</span><br><span class="line"> heap和index的页面填充率，减小填充率，避免update时跨页面，以及index 过多的split页面</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;gp7-写入高问题分析&quot;&gt;&lt;a href=&quot;#gp7-写入高问题分析&quot; class=&quot;headerlink&quot; title=&quot;gp7 写入高问题分析&quot;&gt;&lt;/a&gt;gp7 写入高问题分析&lt;/h1&gt;&lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h1&gt;&lt;p&gt;对比gp7单master 与pg9.6 测试发现一个问题：gp7的磁盘写入量是pg9.6的好几倍，具体表现为：iostat时发现gp7的每秒写入量是pg9.6的好几倍，还有就是查看gplog发现在10分钟的测试期间gp做了2次由于xlog触发的checkpoint，而pg未触发。&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum7" scheme="https://zisedeqing.github.io/categories/greenplum7/"/>
    
    <category term="performance" scheme="https://zisedeqing.github.io/categories/greenplum7/performance/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="IO" scheme="https://zisedeqing.github.io/tags/IO/"/>
    
    <category term="磁盘" scheme="https://zisedeqing.github.io/tags/%E7%A3%81%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>Postgresql 使用Jdbc 启动wal sender进程</title>
    <link href="https://zisedeqing.github.io/2020/12/29/Postgresql-%E4%BD%BF%E7%94%A8Jdbc-%E5%90%AF%E5%8A%A8wal-sender%E8%BF%9B%E7%A8%8B/"/>
    <id>https://zisedeqing.github.io/2020/12/29/Postgresql-%E4%BD%BF%E7%94%A8Jdbc-%E5%90%AF%E5%8A%A8wal-sender%E8%BF%9B%E7%A8%8B/</id>
    <published>2020-12-29T07:54:05.000Z</published>
    <updated>2020-12-29T07:57:43.463Z</updated>
    
    <content type="html"><![CDATA[<p>链接字符串如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">static String url &#x3D; &quot;jdbc:postgresql:&#x2F;&#x2F;10.101.194.174:52540&#x2F;postgres?user&#x3D;test&amp;password&#x3D;123456&quot; +</span><br><span class="line">        &quot;&amp;replication&#x3D;database&quot; + &#x2F;&#x2F; 启动流复制的标志</span><br><span class="line">        &quot;&amp;assumeMinServerVersion&#x3D;090401&quot; +  &#x2F;&#x2F; 必须加上服务器版本信息，至少9.4</span><br><span class="line">        &quot;&amp;options&#x3D;-c gp_session_role&#x3D;utility&quot; + &#x2F;&#x2F; gp必须以utility方式</span><br><span class="line">        &quot;&amp;preferQueryMode&#x3D;simple&quot;; &#x2F;&#x2F; 必须是simple协议</span><br></pre></td></tr></table></figure><a id="more"></a><p>如何执行流复制的命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ClassNotFoundException, SQLException </span>&#123;</span><br><span class="line">    Class.forName(<span class="string">&quot;org.postgresql.Driver&quot;</span>);</span><br><span class="line">    conn = DriverManager.getConnection(url);</span><br><span class="line">    conn.setAutoCommit(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    Statement stmt = conn.createStatement();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 输入replication command</span></span><br><span class="line">    ResultSet rs = stmt.executeQuery(<span class="string">&quot;IDENTIFY_SYSTEM&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (rs != <span class="keyword">null</span> &amp;&amp; rs.next())</span><br><span class="line">        System.out.println(<span class="string">&quot;IDENTIFY_SYSTEM: &quot;</span> + rs.getString(<span class="number">1</span>));</span><br><span class="line">    System.out.println(<span class="string">&quot;connect success&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出如下:<br><img src="/2020/12/29/Postgresql-%E4%BD%BF%E7%94%A8Jdbc-%E5%90%AF%E5%8A%A8wal-sender%E8%BF%9B%E7%A8%8B/output.png" alt="output"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;链接字符串如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;static String url &amp;#x3D; &amp;quot;jdbc:postgresql:&amp;#x2F;&amp;#x2F;10.101.194.174:52540&amp;#x2F;postgres?user&amp;#x3D;test&amp;amp;password&amp;#x3D;123456&amp;quot; +&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;quot;&amp;amp;replication&amp;#x3D;database&amp;quot; + &amp;#x2F;&amp;#x2F; 启动流复制的标志&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;quot;&amp;amp;assumeMinServerVersion&amp;#x3D;090401&amp;quot; +  &amp;#x2F;&amp;#x2F; 必须加上服务器版本信息，至少9.4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;quot;&amp;amp;options&amp;#x3D;-c gp_session_role&amp;#x3D;utility&amp;quot; + &amp;#x2F;&amp;#x2F; gp必须以utility方式&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;quot;&amp;amp;preferQueryMode&amp;#x3D;simple&amp;quot;; &amp;#x2F;&amp;#x2F; 必须是simple协议&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="postgresql" scheme="https://zisedeqing.github.io/categories/postgresql/"/>
    
    <category term="jdbc" scheme="https://zisedeqing.github.io/categories/postgresql/jdbc/"/>
    
    
    <category term="jdbc" scheme="https://zisedeqing.github.io/tags/jdbc/"/>
    
    <category term="postgresql" scheme="https://zisedeqing.github.io/tags/postgresql/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum wait xid问题</title>
    <link href="https://zisedeqing.github.io/2020/12/29/Greenplum-wait-xid%E9%97%AE%E9%A2%98/"/>
    <id>https://zisedeqing.github.io/2020/12/29/Greenplum-wait-xid%E9%97%AE%E9%A2%98/</id>
    <published>2020-12-29T07:42:04.000Z</published>
    <updated>2020-12-29T07:46:51.664Z</updated>
    
    <content type="html"><![CDATA[<h1 id="重现步骤"><a href="#重现步骤" class="headerlink" title="重现步骤"></a>重现步骤</h1><p>数据准备</p><a id="more"></a><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">postgres<span class="operator">=</span># <span class="keyword">create</span> <span class="keyword">table</span> t2 (a <span class="type">int</span>, b <span class="type">int</span>);</span><br><span class="line">NOTICE:  <span class="keyword">Table</span> doesn<span class="string">&#x27;t have &#x27;</span>DISTRIBUTED <span class="keyword">BY</span><span class="string">&#x27; clause -- Using column named &#x27;</span>a<span class="string">&#x27; as the Greenplum Database data distribution key for this table.</span></span><br><span class="line"><span class="string">HINT:  The &#x27;</span>DISTRIBUTED <span class="keyword">BY</span><span class="string">&#x27; clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.</span></span><br><span class="line"><span class="string">CREATE TABLE</span></span><br><span class="line"><span class="string">postgres=# insert into t2 values (1,1),(2,2),(3,3),(4,4),(5,5);</span></span><br><span class="line"><span class="string">INSERT 0 5</span></span><br><span class="line"><span class="string">postgres=# select gp_segment_id, * from t2;</span></span><br><span class="line"><span class="string"> gp_segment_id | a | b</span></span><br><span class="line"><span class="string">---------------+---+---</span></span><br><span class="line"><span class="string">             0 | 2 | 2</span></span><br><span class="line"><span class="string">             0 | 3 | 3</span></span><br><span class="line"><span class="string">             0 | 4 | 4</span></span><br><span class="line"><span class="string">             1 | 1 | 1</span></span><br><span class="line"><span class="string">             2 | 5 | 5</span></span><br><span class="line"><span class="string">(5 rows)</span></span><br></pre></td></tr></table></figure><p>gdb shell</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对db session1的qd process，做断点</span><br><span class="line">$gdb -p 159082</span><br><span class="line">(gdb) b notifyCommittedDtxTransaction</span><br><span class="line">Breakpoint 1 at 0xb4b340: file cdbtm.c, line 326.</span><br></pre></td></tr></table></figure><p>db session 1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">postgres&#x3D;# update t2 set b &#x3D;b+1 where  gp_segment_id&#x3D;0; &lt;-- hang</span><br></pre></td></tr></table></figure><p>在gdb shell中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(gdb) finish</span><br><span class="line">Run till exit from #0  notifyCommittedDtxTransaction () at cdbtm.c:326</span><br><span class="line">CommitTransaction () at xact.c:2900</span><br><span class="line">2900      ClearTransactionState(latestXid);</span><br></pre></td></tr></table></figure><p>db session2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">postgres&#x3D;# update t2 set b &#x3D;b+1 where  gp_segment_id&#x3D;0;</span><br><span class="line">UPDATE 3</span><br></pre></td></tr></table></figure><p>db session3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">postgres&#x3D;# select * from t2;</span><br><span class="line"> a | b</span><br><span class="line">---+---</span><br><span class="line"> 5 | 5</span><br><span class="line"> 1 | 1</span><br><span class="line"> 2 | 2</span><br><span class="line"> 3 | 3</span><br><span class="line"> 4 | 4</span><br><span class="line"> 2 | 4</span><br><span class="line"> 3 | 5</span><br><span class="line"> 4 | 6</span><br><span class="line">(8 rows)</span><br></pre></td></tr></table></figure><p>在gdb shell中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) quit</span><br><span class="line">A debugging session is active.</span><br></pre></td></tr></table></figure><p>db session3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">postgres&#x3D;# select * from t2;</span><br><span class="line"> a | b</span><br><span class="line">---+---</span><br><span class="line"> 1 | 1</span><br><span class="line"> 5 | 5</span><br><span class="line"> 2 | 4</span><br><span class="line"> 3 | 5</span><br><span class="line"> 4 | 6</span><br><span class="line">(5 rows)</span><br></pre></td></tr></table></figure><h1 id="社区版最新修改"><a href="#社区版最新修改" class="headerlink" title="社区版最新修改"></a>社区版最新修改</h1><p>MR: <a href="https://github.com/greenplum-db/gpdb/pull/9710">https://github.com/greenplum-db/gpdb/pull/9710</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;重现步骤&quot;&gt;&lt;a href=&quot;#重现步骤&quot; class=&quot;headerlink&quot; title=&quot;重现步骤&quot;&gt;&lt;/a&gt;重现步骤&lt;/h1&gt;&lt;p&gt;数据准备&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="transaction" scheme="https://zisedeqing.github.io/categories/greenplum6-0/transaction/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="distributed transaction" scheme="https://zisedeqing.github.io/tags/distributed-transaction/"/>
    
    <category term="xid" scheme="https://zisedeqing.github.io/tags/xid/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum GetSnapshot</title>
    <link href="https://zisedeqing.github.io/2020/12/29/Greenplum-GetSnapshot/"/>
    <id>https://zisedeqing.github.io/2020/12/29/Greenplum-GetSnapshot/</id>
    <published>2020-12-29T07:34:54.000Z</published>
    <updated>2020-12-29T07:36:59.563Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GetSnapshotData部分流程"><a href="#GetSnapshotData部分流程" class="headerlink" title="GetSnapshotData部分流程"></a>GetSnapshotData部分流程</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (snapshot-&gt;xip == <span class="literal">NULL</span>)</span><br><span class="line">    init snapshot-&gt;xip;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (snapshot-&gt;subxip == <span class="literal">NULL</span>)</span><br><span class="line">    init snapshot-&gt;subxip;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (distributedsnapshot-&gt;inProgressXidArray == <span class="literal">NULL</span>)</span><br><span class="line">    init distributedsnapshot</span><br><span class="line">    <span class="built_in">malloc</span> distributedsnapshot-&gt;inProgressXidArray</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> (in QE_READ <span class="keyword">or</span> QE_ENTRY_DB_SINGLETON)</span><br><span class="line">    get snapshot from shared snapshot;</span><br><span class="line"></span><br><span class="line">LWLockAcquire(ProcArrayLock, LW_SHARED);</span><br><span class="line"></span><br><span class="line">xmax = ShmemVariableCache-&gt;latestCompletedXid + <span class="number">1</span>;</span><br><span class="line">globalxmin = xmin = xmax;</span><br><span class="line"></span><br><span class="line">foreach proc</span><br><span class="line">&#123;</span><br><span class="line">globalxmin = min(globalxmin, pgxact-&gt;xmin);</span><br><span class="line">xmin = min(xmin, pgxact-&gt;xid);</span><br><span class="line">    </span><br><span class="line">    add to runing xid <span class="built_in">array</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MyPgXact-&gt;xmin = TransactionXmin = xmin;</span><br><span class="line">LWLockRelease(ProcArrayLock);</span><br><span class="line"></span><br><span class="line">globalxmin = min(globalxmin, xmin);</span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">PGXACT</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">TransactionId xid;<span class="comment">/* id of top-level transaction currently being</span></span><br><span class="line"><span class="comment"> * executed by this proc, if running and XID</span></span><br><span class="line"><span class="comment"> * is assigned; else InvalidTransactionId */</span></span><br><span class="line"></span><br><span class="line">TransactionId xmin;<span class="comment">/* minimal running XID as it was when we were</span></span><br><span class="line"><span class="comment"> * starting our xact, excluding LAZY VACUUM:</span></span><br><span class="line"><span class="comment"> * vacuum must not remove tuples deleted by</span></span><br><span class="line"><span class="comment"> * xid &gt;= xmin ! */</span></span><br><span class="line"></span><br><span class="line">uint8vacuumFlags;<span class="comment">/* vacuum-related flags, see above */</span></span><br><span class="line"><span class="keyword">bool</span>overflowed;</span><br><span class="line"><span class="keyword">bool</span>delayChkpt;<span class="comment">/* true if this proc delays checkpoint start;</span></span><br><span class="line"><span class="comment"> * previously called InCommit */</span></span><br><span class="line"></span><br><span class="line">uint8nxids;</span><br><span class="line">&#125; PGXACT;</span><br></pre></td></tr></table></figure><p>PGXACT-&gt;xid:<br>当前backend的xid，如果有正在运行的事务，则为该事务的xid，否则就为0<br>PGXACT-&gt;xmin:<br>当前backend中的事务启动后，系统的xmin，这个xmin是有可能是一个过去式的，仅仅表示当前事务启动之后的xmin。这里的xmin是说小于该值的事务都是已经结束了的事务。</p><p>snapshot要确定3个值：</p><ol><li>xmin</li></ol><p>&lt; xmin的事务修改的数据，对我而言一定是可见的</p><ol start="2"><li>xmax</li></ol><blockquote><p>= xmax的事务修改的数据，对我一定是不可见的<br>xmax = ShmemVariableCache-&gt;latestCompletedXid + 1;</p></blockquote><p>xmax的值是当前时间点最近提交的一个事务xid + 1。</p><ul><li>比xmax小的事务，不确定是否提交了，需要运行时判断</li><li>比xmax大的事务，一定不可见，一定为提交</li></ul><ol start="3"><li>xips</li></ol><p>几个session级别的全局变量：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TransactionXmin：</span><br><span class="line">RecentXmin：</span><br><span class="line">生成snapshot时的xmin，用来实现某些ddl的。对于那些需要创建新的heap，然后交换物理文件的ddl，</span><br><span class="line">    该值用来确定新的heap的frozenxid</span><br><span class="line">RecentGlobalXmin：</span><br><span class="line">当前事务启动后的global xmin, 会考虑vacuum_defer_cleanup_age，replication slot等</span><br><span class="line">    heap_page_prune_opt时，用来vacuum 用户表的</span><br><span class="line">RecentGlobalDataXmin：</span><br><span class="line">与RecentGlobalXmin基本一直，不同之处是会排除replication slot对于catalog的问题</span><br><span class="line">    heap_page_prune_opt时，用来vacuum catalog表的</span><br></pre></td></tr></table></figure><p>globalxmin 与xmin：<br>global xmin是所有session中最小的xmin，而xmin是当前session中，global xmin一般用来实现vacuum的，防止过早的清理tuple。globalxmin的计算与GetOldestXmin中的实现逻辑基本是一样的。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;GetSnapshotData部分流程&quot;&gt;&lt;a href=&quot;#GetSnapshotData部分流程&quot; class=&quot;headerlink&quot; title=&quot;GetSnapshotData部分流程&quot;&gt;&lt;/a&gt;GetSnapshotData部分流程&lt;/h1&gt;&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (snapshot-&amp;gt;xip == &lt;span class=&quot;literal&quot;&gt;NULL&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    init snapshot-&amp;gt;xip;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (snapshot-&amp;gt;subxip == &lt;span class=&quot;literal&quot;&gt;NULL&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    init snapshot-&amp;gt;subxip;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (distributedsnapshot-&amp;gt;inProgressXidArray == &lt;span class=&quot;literal&quot;&gt;NULL&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    init distributedsnapshot&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;malloc&lt;/span&gt; distributedsnapshot-&amp;gt;inProgressXidArray&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (in QE_READ &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; QE_ENTRY_DB_SINGLETON)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    get snapshot from shared snapshot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;LWLockAcquire(ProcArrayLock, LW_SHARED);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;xmax = ShmemVariableCache-&amp;gt;latestCompletedXid + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;globalxmin = xmin = xmax;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;foreach proc&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	globalxmin = min(globalxmin, pgxact-&amp;gt;xmin);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	xmin = min(xmin, pgxact-&amp;gt;xid);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    add to runing xid &lt;span class=&quot;built_in&quot;&gt;array&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MyPgXact-&amp;gt;xmin = TransactionXmin = xmin;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;LWLockRelease(ProcArrayLock);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;globalxmin = min(globalxmin, xmin);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="transaction" scheme="https://zisedeqing.github.io/categories/greenplum6-0/transaction/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="distributed transaction" scheme="https://zisedeqing.github.io/tags/distributed-transaction/"/>
    
    <category term="snapshot" scheme="https://zisedeqing.github.io/tags/snapshot/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum 分布式事务原理</title>
    <link href="https://zisedeqing.github.io/2020/12/29/Greenplum-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/"/>
    <id>https://zisedeqing.github.io/2020/12/29/Greenplum-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/</id>
    <published>2020-12-29T07:32:41.000Z</published>
    <updated>2021-01-21T08:02:03.196Z</updated>
    
    <content type="html"><![CDATA[<p>CommitTransaction<br>prepareDtxTransaction<br>RecordTransactionCommit<br>insert XLOG_XACT_DISTRIBUTED_COMMIT/XLOG_XACT_COMMIT log</p><a id="more"></a><p>notifyCommittedDtxTransaction</p><p>pg_prepared_xact<br>TwoPhaseState<br>ProcGlobal-&gt;allProcs<br>ProcGlobal-&gt;allPgXact</p><p>gp_distributed_xacts<br>allTmGxact<br>allTmGxact = ProcGlobal-&gt;allTmGxact;</p><h1 id="master-recoverTM"><a href="#master-recoverTM" class="headerlink" title="master recoverTM"></a>master recoverTM</h1><p>recoverInDoubtTransactions<br>对于已经有distributed commit log的事务，执行recovery commit prepared<br>从所有的segment上收集已经prepared 但是没有end的事务<br>对这些事务执行recovery abort prepared</p><h1 id="segment-recover"><a href="#segment-recover" class="headerlink" title="segment recover"></a>segment recover</h1><p>startupXLOG redo所有segment上的xlog，对于分布式事务，segment上有3种xlog：XLOG_XACT_PREPARE、XLOG_XACT_COMMIT_PREPARED、XLOG_XACT_ABORT_PREPARED<br>XLOG_XACT_PREPARE：<br>把已经完成prepared 事务保存到crashRecoverPostCheckpointPreparedTransactions_map_ht<br>XLOG_XACT_COMMIT_PREPARED：<br>redo xlog，并把事务从crashRecoverPostCheckpointPreparedTransactions_map_ht中删除<br>XLOG_XACT_ABORT_PREPARED：<br>redo xlog，并把事务从crashRecoverPostCheckpointPreparedTransactions_map_ht中删除</p><p>在startupXLOG redo完所有的xlog后，调用RecoverPreparedTransactions，把没有提交或者回滚的prepared 事务标记为已经prepared，也就是修改TwoPhaseState。</p><h1 id="GetSnapshotData"><a href="#GetSnapshotData" class="headerlink" title="GetSnapshotData"></a>GetSnapshotData</h1><p>参考<a href="/2020/12/29/Greenplum-GetSnapshot/index.html">GetSnapshotData</a></p><h1 id="DistributedSnapshotWithLocalMapping"><a href="#DistributedSnapshotWithLocalMapping" class="headerlink" title="DistributedSnapshotWithLocalMapping"></a>DistributedSnapshotWithLocalMapping</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DistributedSnapshot</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * The unique timestamp for this start of the DTM.  It applies to all of</span></span><br><span class="line"><span class="comment"> * the distributed transactions in this snapshot.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">DistributedTransactionTimeStampdistribTransactionTimeStamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * The lowest distributed transaction being used for distributed snapshots.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">DistributedTransactionId xminAllDistributedSnapshots;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Unique number identifying this particular distributed snapshot.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">DistributedSnapshotId distribSnapshotId;</span><br><span class="line"></span><br><span class="line">DistributedTransactionId xmin;<span class="comment">/* XID &lt; xmin are visible to me */</span></span><br><span class="line">DistributedTransactionId xmax;<span class="comment">/* XID &gt;= xmax are invisible to me */</span></span><br><span class="line">int32count;<span class="comment">/*  # of distributed xids in inProgressXidArray */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Array of distributed transactions in progress. */</span></span><br><span class="line">DistributedTransactionId        *inProgressXidArray;</span><br><span class="line">&#125; DistributedSnapshot;</span><br></pre></td></tr></table></figure><p>DistributedSnapshotWithLocalMapping 由2部分组成，一是DistributedSnapshot，用来记录分布式的snapshot，二是一个cache，用来快速判断分布式事务的。第一部分就不说了，主要说一下第二部分。<br>由于分布式事务可见性的cache分了2层，第一层是DistributedSnapshotWithLocalMapping里面除了DistributedSnapshot里面的部分; 第二层是LocalDistribCacheHtab。下面分别介绍，代码可以参考DistributedSnapshotWithLocalMapping_CommittedTest。</p><h2 id="第一层cache"><a href="#第一层cache" class="headerlink" title="第一层cache"></a>第一层cache</h2><p>该层cache是snapshot级别的，在生成新的snapshot时即清空，在查询过程中通过事务可见性判断不断的添加。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DistributedSnapshotWithLocalMapping</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">DistributedSnapshot ds;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Cache to perform quick check for localXid, populated after reverse</span></span><br><span class="line"><span class="comment"> * mapping distributed xid to local xid.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">TransactionId minCachedLocalXid;</span><br><span class="line">TransactionId maxCachedLocalXid;</span><br><span class="line">int32 currentLocalXidsCount;</span><br><span class="line">TransactionId *inProgressMappedLocalXids;</span><br><span class="line">&#125; DistributedSnapshotWithLocalMapping;</span><br></pre></td></tr></table></figure><ol><li>inProgressMappedLocalXids：对当前snapshot而言，正在运行的分布式事务对应的local xid的数组。currentLocalXidsCount是数组的长度。之所以记录local xid，是因为tuple上记录的就是local xid。要想跟进gxid找到对应的local xid，必须通过dlog。</li><li>minCachedLocalXid和maxCachedLocalXid分别是inProgressMappedLocalXids里面的最小值和最大值</li></ol><p>如果待测试的xid 在 (minCachedLocalXid, maxCachedLocalXid) 范围内，则可以遍历inProgressMappedLocalXids数组，如果xid在inProgressMappedLocalXids内，则说明事务一定是inprogress的。否则的话需要做进一步的判断。</p><p><strong>该层cache的限制：</strong></p><ol><li><strong>对rr隔离级别或者rc级别单条sql扫描比较多tuple时，可能效果会明显一些</strong></li></ol><p><strong>原因是cache的有效期是snapshot级别的，对于rr隔离级别，snapshot是整个事务，可能会使用的比较多</strong><br><strong>对于rc由于snapshot是SQL级别的，所以如果涉及的tuple比较多，则也可能会有效果。</strong></p><ol start="2"><li><strong>inProgressMappedLocalXids实际上是inProgressXidArray的一个子集，期望是当前事务扫描的tuple涉及的事务比inProgressXidArray里面的事务要少，这样才可能会有加速效果，否则也不会有太多效果。</strong></li></ol><h2 id="第二层cache"><a href="#第二层cache" class="headerlink" title="第二层cache"></a>第二层cache</h2><p>该层cache是session级别的，每个session会维护一个hashmap，记录已经确定提交了的分布式事务信息。记录的信息如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">LocalDistribXactCacheEntry</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Distributed and local xids.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">TransactionId localXid;</span><br><span class="line"><span class="comment">/* MUST BE FIRST: Hash table key. */</span></span><br><span class="line"></span><br><span class="line">DistributedTransactionId distribXid;</span><br><span class="line"></span><br><span class="line">int64visits;</span><br><span class="line"></span><br><span class="line">dlist_nodelruDoubleLinks;</span><br><span class="line"><span class="comment">/* list link for LRU */</span></span><br><span class="line"></span><br><span class="line">&#125; LocalDistribXactCacheEntry;</span><br></pre></td></tr></table></figure><ol><li>localXid： segment本地xid</li><li>distribXid： 已经提交的分布式gxid</li><li>visits：访问计数器</li></ol><p>在通过第一层cache无法判断事务可见性时，会使用该cache进行进一步的判断：</p><ol><li>根据localXid 查找是否在LocalDistribCacheHtab中，如果在LocalDistribCacheHtab中，则表示该分布式事务已经提交了，然后在通过分布式snapshot 判断tuple是否可见。</li><li>如果不在LocalDistribCacheHtab中，则查找distributed log</li><li>如果在distributed log中存在<ol><li>如果timestamp不同，则返回DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE</li><li>否则，将该事务信息添加到LocalDistribCacheHtab中，然后根据分布式snapshot判断可见性。</li></ol></li><li>如果在distributed log中不存在，以local-only方式添加到LocalDistribCacheHtab中，并返回DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE</li></ol><p>根据事务可见性判断的顺序，到这一步的时候，local xid是一定已经提交了的，而distributed log里面却没有，则这种情况就定义为local-only。local-only是说该事务是segment本地事务，没有gxid与之对应，所以distribXid = InvalidDistributedTransactionId。<br><img src="/2020/12/29/Greenplum-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/local-only.png" alt="image.png"></p><p>该层cache是session级别的，只记录已经提交过的分布式事务信息。</p><p><strong>问题：</strong></p><ul><li><input checked disabled type="checkbox"> <strong>为什么在local xid已经提交，但是dlog里面没有提交记录时，断定一定是local-only？</strong></li></ul><p>因为gp在事务提交时，segment上是先记录distributed log，然后在记录clog。</p><ul><li><input checked disabled type="checkbox"> <strong>哪些是local-only事务？</strong></li></ul><p>local-only的事务是那些只有local  xid，但是却没有gxid与之对应的事务，目前想到的场景是：</p><ol><li>utility 链接segment，然后执行的事务</li><li>分布式事务，但是dlog已经被truncate了，通过local xid也无法找到对应的gxid</li></ol><p>这2中情况，在事务可见性判断时，就可以只用local snapshot来做。</p><h1 id="dxid-与local-xid的对应关系"><a href="#dxid-与local-xid的对应关系" class="headerlink" title="dxid 与local xid的对应关系"></a>dxid 与local xid的对应关系</h1><p>gp中有2个xid，分别是dxid和local xid，其中dxid是分布式xid，由master生成和维护，local xid是segment本地的xid，有segment生成和维护。为了实现分布式事务，gp通过distributed log把gxid和local xid映射起来。<br>distributed log的记录位置有local xid计算出来，并把timestamp 和 dxid记录到dlog中。映射关系可以查看DistributedLog_AdvanceOldestXmin相关代码。<br>比如在计算segment的oldest xmin时，先在GetSnapshotData里面计算本地局部最小xmin，然后调用DistributedLog_AdvanceOldestXmin和snapshot传递的全局最小的dxid，通过查找dlog找到全局事务最小的本地xmin，代码如下：<br><img src="/2020/12/29/Greenplum-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/DistributedLog_AdvanceOldestXmin.png" alt="image.png"></p><h1 id="HeapTupleSatisfiesMVCC"><a href="#HeapTupleSatisfiesMVCC" class="headerlink" title="HeapTupleSatisfiesMVCC"></a>HeapTupleSatisfiesMVCC</h1><p>对比了一下HeapTupleSatisfiesMVCC的整体流程，与pg9.4几乎一样，除了在处理distributed snapshot时添加了一个DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE hint之外。<br>pg中可见性判断的顺序是，先判断tuple的xmin和xmax是否提交，如果没有提交一定是不可见的，如果 提交了，则再根据snapshot判断，对于这个snapshot是否可见。<br>gp引入dxid和distributed snapshot后，判断的顺序就变为了：</p><ol><li>tuple的xmin和xmax是否已经提交，如果没有提交则一定不可见</li><li>如果已经提交，然后是判断对应的dxid是否已经提交，如果没有提交，则一定不可见</li><li>如果已经提交，根据distributed snapshot判断是否可见</li><li>最后，该分布式事务在”很早之前”已经结束，则直接根据local snapshot判断是否可见</li></ol><p>XidInMVCCSnapshot：</p><ol><li>如果有distributed snapshot &amp;&amp; distributed snapshot check不能忽略则：<ol><li>调用DistributedSnapshotWithLocalMapping_CommittedTest判断分布式snapshot，确定tuple全局是否可见</li><li>DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS：表示分布式事务正在进行中，tuple不可见</li><li>DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE：表示分布式事务已经结束，并且tuple可见</li><li>DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE：表示可以忽略分布式事务的判断，只需要执行本地事务判断即可。</li></ol></li></ol><p>哪些事务会返回DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE？</p><ol start="2"><li>调用XidInMVCCSnapshot_Local进行本地事务判断</li></ol><ul><li><input checked disabled type="checkbox"> <strong>DistributedSnapshotWithLocalMapping_CommittedTest，由于tuple记录的还是local xid，所以最终判断时，还是要转换成local xid进行判断，流程如下：</strong></li></ul><ol><li>如果local xid is not normal，则返回DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE</li><li>如果第一层cache不为空，则根据第一层cache判断：<ol><li>如果local xid == minCachedLocalXid || local xid == maxCachedLocalXid, 则返回DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS</li><li>如果local xid 在(minCachedLocalXid, maxCachedLocalXid) 范围内，并且在inProgressMappedLocalXids数组内，则返回DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS</li><li>否则进行下一步的判断</li></ol></li><li>根据local xid在第二层cache进行判断<ol><li>如果在LocalDistribCacheHtab中找到，且对应的distribXid为0，则返回DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS;否则，进行下一步的判断</li><li>如果LocalDistribCacheHtab中没有找到：<ol><li>读取dlog，如果在dlog中找到，并且timestamp不同，则返回DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE</li><li>否则把该事务的分布式信息记录到LocalDistribCacheHtab中，然后进行下一步的判断</li><li>如果dlog中没有找到，则说明事务是local-only事务，记录到LocalDistribCacheHtab中，然后进行下一步的判断</li></ol></li></ol></li><li>如果对于的distribXid &lt; xminAllDistributedSnapshots,则返回DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE</li><li>如果是在vacuum时调用的，则返回DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS<ol><li>看了一下代码，该情况目前不会发生了。</li></ol></li><li>distribXid &lt; ds-&gt;xmin， 返回DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE</li><li>distribXid &gt;= ds-&gt;xmax，返回DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS</li><li>遍历inProgressXidArray， 如果找到，则维护第一层cache，并返回DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS</li><li>否则，返回DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE</li></ol><ul><li><input checked disabled type="checkbox"> <strong>XidInMVCCSnapshot_Local：</strong></li></ul><p><strong>与单机pg一样。这里不在说明了。</strong></p><p><strong>QA：</strong></p><ol><li><strong>哪些情况还需要判断local snapshot？</strong><ol><li>事务可见性判断时，是先判断distributed snapshot，然后在local snapshot，需要再次判断local snapshot的情况是：一是没有distributed snapshot; 而是在判断distributed snapshot时返回了DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE。</li></ol></li><li><strong>DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE的含义到底是什么？</strong><ol><li>哪些事务可见性判断时会出现DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE：<ol><li>local xid is not normal: 这种事务是一定没有gxid与之对应的<ol><li>InvalidTransactionId</li><li>BootstrapTransactionId</li><li>FrozenTransactionId</li></ol></li><li>local xid 对应的gxid 是InvalidDistributedTransactionId：这个就是local-only 事务</li><li>local xid 对应的timestamp 与当前snapshot timestamp不同：说明该事务一定是在重启之前开启的，则该事务一定已经结束，可以按local snapshot为准</li><li>local only事务：参考DistributedSnapshotWithLocalMapping中的分析，该情况也可以按local snapshot为准。</li><li>gxid &lt; xminAllDistributedSnapshots: xminAllDistributedSnapshots表示当前正在运行的分布式事务中gxid最小的事务。小于该值的事务表示肯定是已经结束了的事务。因此也可以按local 事务处理，只判断local snapshot。</li></ol></li><li>从上面的分析看，DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE的含义表示该事务不是分布式事务或者是分布式事务，但是对应的分布式事务在”很早之前”就已经结束，不用关注该事务的分布式特性，只需要通过local 事务就可以做出正确的判断。</li><li><strong>这个操作依赖于2个必要条件：</strong><ol><li><strong>分布式事务的最终状态一定与所有segment的local事务最终状态是一样的。</strong></li><li><strong>该分布式事务，一定与现在正在运行的分布式事务没有任何运行时间上的交集。</strong></li></ol></li></ol></li><li><strong>为什么是先判断distributed snapshot，然后才可能会在判断local snapshot？</strong></li></ol><p>gp是先segment 提交，然后在master提交，所以如果先distributed snapshot是准确的，而local snapshot是不准的。从上面的分析也可以看到，只有在已经确定分布式事务与现有所有的分布式事务没有任何交集时，才会以local snapshot为准。</p><h1 id="wait-xids"><a href="#wait-xids" class="headerlink" title="wait xids"></a>wait xids</h1><p>wait xids要解决的问题：<a href="/2020/12/29/Greenplum-wait-xid%E9%97%AE%E9%A2%98/index.html">wait xid</a><br>实现的原理是：</p><ol><li>在segment上执行sql时，如果发现有事务冲突，则记录冲突的xid，代码在XactLockTableWait</li><li>在segment执行完commit prepared事务后，把本地记录的wait xids 发给master，代码在：performDtxProtocolCommitOnePhase/performDtxProtocolCommitPrepared</li><li>master在所有的segment执行完commit prepared事务后，等待依赖的事务完成提交，代码在notifyCommittedDtxTransaction</li><li>然后master提交该事务。</li></ol><p>问题：<br><strong>在segment上，wait xids记录在topmemory context，具体参考代码XactLockTableWait，清理该链表是在ProcArrayEndTransaction函数内，那么如果在ProcArrayEndTransaction之前事务回滚了，是不是会造成内存泄漏？</strong><br>好像不会</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol><li>如何判断pg_prepared_xact的事务是残留的，还是正常的？</li></ol><ul><li>根据pg_prepared_xact的gid，在所有正常的master上找gp_distributed_xacts，如果找到，根据sid，判断发起该事务的session是否还活着，如果活着，则是正常的2pc事务。否则为残留的2pc事务。</li></ul><ol start="2"><li>为什么recoverInDoubtTransactions中会收集所有segment上为提交的prepared事务？<br>a. master已经记录distributed commit log，但是还未同步到standby，这时发生切主<br>b. segment 已经完成prepared，但是master还未记录distributed commit log，master 挂掉了<br>c. 部分segment prepared成功，部分失败，master在rollback时，或者过程中挂掉了</li></ol><ul><li><input disabled type="checkbox"> xminAllDistributedSnapshots的计算<br>xminAllDistributedSnapshots是在CreateDistributedSnapshot里面计算的，含义是当前活跃事务中dxid最小的那个。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;CommitTransaction&lt;br&gt;prepareDtxTransaction&lt;br&gt;RecordTransactionCommit&lt;br&gt;insert XLOG_XACT_DISTRIBUTED_COMMIT/XLOG_XACT_COMMIT log&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="transaction" scheme="https://zisedeqing.github.io/categories/greenplum6-0/transaction/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="distributed transaction" scheme="https://zisedeqing.github.io/tags/distributed-transaction/"/>
    
  </entry>
  
  <entry>
    <title>64 bits GXID</title>
    <link href="https://zisedeqing.github.io/2020/12/29/64-bits-GXID/"/>
    <id>https://zisedeqing.github.io/2020/12/29/64-bits-GXID/</id>
    <published>2020-12-29T03:31:27.000Z</published>
    <updated>2020-12-29T06:59:33.634Z</updated>
    
    <content type="html"><![CDATA[<p>社区的MR:<a href="https://github.com/greenplum-db/gpdb/pull/10910">https://github.com/greenplum-db/gpdb/pull/10910</a><br>distributed snapshot膨胀：使用相对值，依然32位<br>dlog膨胀：去掉timestamp，报错和之前一样<br>ShmemVariableCache-&gt;nextGxid在异常宕机、主备切换时的维护：<br>作用于ShmemVariableCache-&gt;nextXid一样。</p><a id="more"></a><p>2种方案：</p><ol><li>与oid类似，按批来处理，每批记一次xlog。社区认为这个代价太大了。</li><li>现在使用的方案<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Another solution (in this patch) is:</span><br><span class="line"></span><br><span class="line">Always update ShmemVariableCache-&gt;nextGxid in checkpoint and one phase</span><br><span class="line">commit&#x2F;prepare (during both commit and redo code)</span><br><span class="line"></span><br><span class="line">in dtx recovery, for the scenarios, it previously terminate all backends in</span><br><span class="line">such case, now it dispatches a udf to collect the max nextGxid on all segments</span><br><span class="line">and compare with ShmemVariableCache-&gt;nextGxid to get the max nextGxid.</span><br><span class="line">Need to write the code very carefully when handling those orphaned segment</span><br><span class="line">processes on segments.</span><br></pre></td></tr></table></figure>与xid类似，记录到xlog中，从xlog里面恢复到正确的值。<br>对于多master，这个需要做一些改动，因为gxid是在gtm上生成的。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;社区的MR:&lt;a href=&quot;https://github.com/greenplum-db/gpdb/pull/10910&quot;&gt;https://github.com/greenplum-db/gpdb/pull/10910&lt;/a&gt;&lt;br&gt;distributed snapshot膨胀：使用相对值，依然32位&lt;br&gt;dlog膨胀：去掉timestamp，报错和之前一样&lt;br&gt;ShmemVariableCache-&amp;gt;nextGxid在异常宕机、主备切换时的维护：&lt;br&gt;作用于ShmemVariableCache-&amp;gt;nextXid一样。&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="transaction" scheme="https://zisedeqing.github.io/categories/greenplum6-0/transaction/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="gxid" scheme="https://zisedeqing.github.io/tags/gxid/"/>
    
    <category term="64位" scheme="https://zisedeqing.github.io/tags/64%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>postgresql jdbc 走simple query protocol</title>
    <link href="https://zisedeqing.github.io/2020/12/29/postgresql-jdbc-%E8%B5%B0simple-query-protocol/"/>
    <id>https://zisedeqing.github.io/2020/12/29/postgresql-jdbc-%E8%B5%B0simple-query-protocol/</id>
    <published>2020-12-29T01:56:00.000Z</published>
    <updated>2020-12-29T02:07:50.659Z</updated>
    
    <content type="html"><![CDATA[<p>默认情况下，jdbc的所有sql使用extended query protocol，不过pg的jdbc提供了使用simple query protocol方法。<br><strong>需要注意的是，使用simple query protocol后，jdbc的cursor功能将会失效，也就是无论setFetchSize设置为多大，jdbc都会一次把数据全部存到jdbc。不过resultset的滚动和update功能还是可以使用的。</strong></p><a id="more"></a><ul><li><input checked disabled type="checkbox"> <strong>connection级别</strong></li></ul><p>在链接字符串中添加preferQueryMode=simple参数，这样connection所有的sql执行使用的都是simple query protocol。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Class.forName(<span class="string">&quot;org.postgresql.Driver&quot;</span>);</span><br><span class="line"><span class="comment">// set preferQueryMode=simple</span></span><br><span class="line">Connection conn = DriverManager.getConnection(<span class="string">&quot;jdbc:postgresql://localhost:15432/postgres?preferQueryMode=simple&quot;</span>, <span class="string">&quot;postgres&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">Statement stmt = conn.createStatement();</span><br><span class="line">ResultSet rs = stmt.executeQuery(<span class="string">&quot;select * from test8 where b = 1&quot;</span>);</span><br><span class="line"><span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">ResultSetMetaData rsm = rs.getMetaData();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= rsm.getColumnCount(); i++) &#123;</span><br><span class="line">System.out.println(rsm.getColumnName(i) + <span class="string">&quot;:&quot;</span> + rs.getString(i));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> <strong>sql级别</strong></li></ul><p>如果只针对某些sql使用simple query protocol，标准jdbc接口是无法实现的，只能通过pg的jdbc接口实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Class.forName(<span class="string">&quot;org.postgresql.Driver&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// preferQueryMode 不能为simpe或者extended,可以为extendedForPrepared或者extendedCacheEverything</span></span><br><span class="line">Connection conn = DriverManager.getConnection(<span class="string">&quot;jdbc:postgresql://localhost:15432/postgres?preferQueryMode=extendedForPrepared&quot;</span>, <span class="string">&quot;postgres&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 强制转换标准Statement为PgStatement</span></span><br><span class="line">PgStatement pgStmt = (PgStatement)conn.createStatement();</span><br><span class="line"><span class="keyword">if</span> (pgStmt.executeWithFlags(<span class="string">&quot;select * from test8 where b = 2&quot;</span>, QueryExecutor.QUERY_EXECUTE_AS_SIMPLE)) &#123;</span><br><span class="line">  ResultSet rs = pgStmt.getResultSet();</span><br><span class="line">  <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">ResultSetMetaData rsm = rs.getMetaData();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= rsm.getColumnCount(); i++) &#123;</span><br><span class="line">System.out.println(rsm.getColumnName(i) + <span class="string">&quot;:&quot;</span> + rs.getString(i));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;默认情况下，jdbc的所有sql使用extended query protocol，不过pg的jdbc提供了使用simple query protocol方法。&lt;br&gt;&lt;strong&gt;需要注意的是，使用simple query protocol后，jdbc的cursor功能将会失效，也就是无论setFetchSize设置为多大，jdbc都会一次把数据全部存到jdbc。不过resultset的滚动和update功能还是可以使用的。&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="postgresql" scheme="https://zisedeqing.github.io/categories/postgresql/"/>
    
    <category term="jdbc" scheme="https://zisedeqing.github.io/categories/postgresql/jdbc/"/>
    
    
    <category term="jdbc" scheme="https://zisedeqing.github.io/tags/jdbc/"/>
    
    <category term="postgresql" scheme="https://zisedeqing.github.io/tags/postgresql/"/>
    
    <category term="simple" scheme="https://zisedeqing.github.io/tags/simple/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum jdbc qps 问题</title>
    <link href="https://zisedeqing.github.io/2020/12/29/Greenplum-jdbc-qps-%E9%97%AE%E9%A2%98/"/>
    <id>https://zisedeqing.github.io/2020/12/29/Greenplum-jdbc-qps-%E9%97%AE%E9%A2%98/</id>
    <published>2020-12-29T01:23:26.000Z</published>
    <updated>2020-12-29T02:07:53.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>目前的现象是使用psql 执行点查，qps比使用jdbc执行相同的sql 要好几倍。<br>在docker里面做了一下测试，下面是测试脚本：</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">pronum=4</span><br><span class="line">dbname=postgres</span><br><span class="line"></span><br><span class="line">wait_end()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">true</span></span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line"><span class="built_in">jobs</span></span><br><span class="line">        running=`<span class="built_in">jobs</span>|wc -l`</span><br><span class="line">        ret=$?</span><br><span class="line">        [ ! <span class="variable">$ret</span> -eq 0 ] &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;check running process failed, error <span class="variable">$ret</span>&quot;</span> &amp;&amp; <span class="built_in">return</span> <span class="variable">$ret</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;running process <span class="variable">$running</span>&quot;</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$running</span> -le 0 ];<span class="keyword">then</span></span><br><span class="line">            <span class="built_in">return</span> 0</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        sleep 1</span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start_tm=`date +%s%N`;</span><br><span class="line"><span class="keyword">for</span>((i=0;i&lt;pronum;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    psql -d <span class="variable">$dbname</span> -f test.sql &amp;&gt;/dev/null &amp;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;waiting job finish ...&quot;</span></span><br><span class="line">wait_end</span><br><span class="line">end_tm=`date +%s%N`;</span><br><span class="line">use_tm=`<span class="built_in">echo</span> <span class="variable">$end_tm</span> <span class="variable">$start_tm</span> | awk <span class="string">&#x27;&#123; print ($1 - $2) / 1000000000&#125;&#x27;</span>`</span><br><span class="line">qps=`<span class="built_in">echo</span> <span class="string">&quot;(<span class="variable">$pronum</span> * 1050) / <span class="variable">$use_tm</span>&quot;</span>|bc`</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;use time: <span class="variable">$use_tm</span> (s), qps: <span class="variable">$qps</span> (q/s)&quot;</span></span><br></pre></td></tr></table></figure><p>test.sql 里面是1000行SELECT * from test8 where b = 1000</p><p>jdbc程序：<a href="test2.tar.gz">test2.tar.gz</a><br>命令行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp .&#x2F;*:.&#x2F;test2-1.0-SNAPSHOT-jar-with-dependencies.jar MainEntry jdbc:postgresql:&#x2F;&#x2F;localhost:15432&#x2F;postgres postgres 111 &quot;select * from test8 where b &#x3D;1000&quot; 1000 4 true</span><br></pre></td></tr></table></figure><p><strong>测试结果：</strong><br>psql:<br>use time: 6.08536 (s), qps: 690 (q/s)<br>jdbc:<br>Total run time: 33209 ms<br>QPS Tester: QPS = [120.44927579872925] querys/s</p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>修改log级别为DEBUG5(set log_min_messages=DEBUG5)，输出更详细的log，比较psql与jdbc执行SELECT * from test8 where b = 1000的差异，对比发现使用jdbc时，有如下log：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DEBUG5:  CdbDoCommand for command &#x3D; &#39;set gp_write_shared_snapshot&#x3D;true&#39;, needTwoPhase &#x3D; true</span><br><span class="line">DEBUG5:  dtmPreCommand going distributed (all gangs) for gid &#x3D; 1554712434-0000012220 (CdbDoCommand, detail &#x3D; &#39;set gp_write_shared_snapshot&#x3D;true&#39;)</span><br><span class="line">DEBUG3:  cdbdisp_dispatchCommand: set gp_write_shared_snapshot&#x3D;true (needTwoPhase &#x3D; true)</span><br><span class="line">DEBUG5:  mppTxnOptions DefaultXactIsoLevel &#x3D; READ COMMITTED, DefaultXactReadOnly &#x3D; false, XactIsoLevel &#x3D; READ COMMITTED, XactReadOnly &#x3D; false.</span><br><span class="line">DEBUG5:  mppTxnOptions txnOptions &#x3D; 0x3, needTwoPhase &#x3D; true, explicitBegin &#x3D; false, isoLevel &#x3D; READ COMMITTED, readOnly &#x3D; false.</span><br></pre></td></tr></table></figure><p>QD 在执行查询之前会给每个segment发送set gp_write_shared_snapshot=true，并且启动two phase commit。<br>代码如下：<br><img src="/2020/12/29/Greenplum-jdbc-qps-%E9%97%AE%E9%A2%98/verify_shared_snapshot_ready.png" alt="verify_shared_snapshot_ready"><br><img src="/2020/12/29/Greenplum-jdbc-qps-%E9%97%AE%E9%A2%98/verify_shared_snapshot_ready.png" alt="verify_shared_snapshot_ready"></p><p>QD对应extended query，在ExecutorStart和cdbdisp_dispatchPlan时，会执行verify_shared_snapshot_ready，该函数会dispatch set gp_write_shared_snapshot=true到每个segment，并使用两阶段提交，拖慢整个执行性能。</p><ul><li><p><strong>问题1：shared snapshot是干什么用的？</strong><br>参考: <a href="/2020/12/28/Greenplum-shared-snapshot/index.html">Greenplum shared snapshot</a></p></li><li><p><strong>问题2：verify_shared_snapshot_ready的作用是什么？</strong></p></li></ul><p>根据注释看，该函数的主要作用是在segment上启动一个writer gang，并且生成shared snapshot。</p><ul><li><input checked disabled type="checkbox"> <strong>问题3：为什么extended query 会比simple query多走这一步？</strong></li></ul><p>根据代码和注释了解，extended query在执行的时候，是没有writer gang的，所有的执行操作都是在reader gang中，但是reader gang也需要snapshot，所以需要执行此操作。</p><ul><li><input checked disabled type="checkbox"> <strong>问题4：问什么extended query没有writer gang？</strong></li></ul><p><img src="/2020/12/29/Greenplum-jdbc-qps-%E9%97%AE%E9%A2%98/assignGangs.png" alt="image.png"><br>从上面的代码可以看到，在assignGangs时，如果是extended query，则跳过writer gang的分配，只分配reader gang。<br><strong>对于cursor，同一个session可以同时执行多个cursor，由于writer gang只能有一个，所以所有的cursor只能在reader gang中执行，并且cursor是不会修改数据的，所以assignGangs没有分配writer gang。</strong></p><p>例子1 psql：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">postgres&#x3D;# BEGIN ;</span><br><span class="line">BEGIN</span><br><span class="line">postgres&#x3D;# DECLARE c1 CURSOR for select a from test8 where b &#x3D;10;</span><br><span class="line">DECLARE CURSOR</span><br><span class="line">postgres&#x3D;# DECLARE c2 CURSOR for select a from test8 where b &#x3D;11;</span><br><span class="line">DECLARE CURSOR</span><br><span class="line">postgres&#x3D;# FETCH c1;</span><br><span class="line"> a</span><br><span class="line">----</span><br><span class="line"> 10</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">postgres&#x3D;# FETCH c2;</span><br><span class="line"> a</span><br><span class="line">----</span><br><span class="line"> 11</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">postgres&#x3D;# CLOSE c1;</span><br><span class="line">CLOSE CURSOR</span><br><span class="line">postgres&#x3D;# close c2;</span><br><span class="line">CLOSE CURSOR</span><br><span class="line">postgres&#x3D;#</span><br></pre></td></tr></table></figure><p>例子2 jdbc:<br>关于jdbc开启cursor的文档：<a href="https://jdbc.postgresql.org/documentation/94/query.html#query-with-cursor">https://jdbc.postgresql.org/documentation/94/query.html#query-with-cursor</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Connection conn = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">static</span> Statement stmt = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> ClassNotFoundException, SQLException </span>&#123;</span><br><span class="line">        Class.forName(<span class="string">&quot;org.postgresql.Driver&quot;</span>);</span><br><span class="line">        conn = DriverManager.getConnection(<span class="string">&quot;jdbc:postgresql://localhost:15432/postgres&quot;</span>, <span class="string">&quot;postgres&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        conn.setAutoCommit(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        Statement st1 = conn.createStatement();</span><br><span class="line">        Statement st2 = conn.createStatement();</span><br><span class="line">        st1.setFetchSize(<span class="number">1</span>);</span><br><span class="line">        st2.setFetchSize(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        ResultSet rs1 = st1.executeQuery(<span class="string">&quot;select a from test8 where b = 1&quot;</span>);</span><br><span class="line">        ResultSet rs2 = st2.executeQuery(<span class="string">&quot;select b from test8 where b = 11&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (rs1.next()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;fetch rs1: &quot;</span> + rs1.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (rs2.next()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;fetch rs2: &quot;</span> + rs2.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (rs1.next()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;fetch rs1: &quot;</span> + rs1.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (rs2.next()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;fetch rs2: &quot;</span> + rs2.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        rs1.close();</span><br><span class="line">        rs2.close();</span><br><span class="line"></span><br><span class="line">        st1.close();</span><br><span class="line">        st2.close();</span><br><span class="line">        conn.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> <strong>问题4：为什么对于extended query，shared snapshot需要dump到file？</strong></li></ul><p>参考: <a href="/2020/12/28/Greenplum-shared-snapshot/index.html">Greenplum shared snapshot</a></p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>有两种解决方式：</p><ul><li><p><strong>业务修改jdbc，让jdbc走simple query protocol</strong><br>参考: <a href="/2020/12/29/postgresql-jdbc-%E8%B5%B0simple-query-protocol/index.html">postgresql jdbc 走simple queryprotocol</a></p></li><li><p><strong>修改gp内存</strong></p></li></ul><p>针对cursor和extended query中的cursor，依然走原来的流程，对于非cursor的extended query走simple query 流程。<br><strong>如何区分cursor和extended query中的cursor？</strong><br>simple query: declare cursor/fetch<br>extended query: 如果是cursor bind时，会有portal name，否则portal name为空</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;目前的现象是使用psql 执行点查，qps比使用jdbc执行相同的sql 要好几倍。&lt;br&gt;在docker里面做了一下测试，下面是测试脚本：&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum4.3" scheme="https://zisedeqing.github.io/categories/greenplum4-3/"/>
    
    <category term="performance" scheme="https://zisedeqing.github.io/categories/greenplum4-3/performance/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="jdbc" scheme="https://zisedeqing.github.io/tags/jdbc/"/>
    
    <category term="qps" scheme="https://zisedeqing.github.io/tags/qps/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum Fts实现</title>
    <link href="https://zisedeqing.github.io/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/"/>
    <id>https://zisedeqing.github.io/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/</id>
    <published>2020-12-28T13:47:20.000Z</published>
    <updated>2020-12-29T03:22:31.723Z</updated>
    
    <content type="html"><![CDATA[<p>Greenplum 总共存在2个角色，每个角色有分别实现了standby，分别是：master和master的standby， primary segment和mirror segment。数据库的fault tolerance由fault tolerance server（简称FTS）来处理。<br>FTS主要监控和控制segment节点的状态，以及发生异常时进行切换，FTS并不监控standby的状态，目前的实现是由管控控制。<br>    FTS 由2类进程组成：ftsprobe 进程和 fts handle message 进程，ftsprobe 进程在master上，定期给segment发送探活消息， fts handle message 进程是segment上处理ftsprobe 进程探活消息的，改进程不是守护进程，处理完一次探活后，即退出。</p><a id="more"></a><p><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/fts.png" alt="fts"><br>如上图所示，fts probe 定期发送探活信息给primary segment， primary segment收到探活信息，并从wal sender 获取mirror segment状态，然后回复给fts probe进程。backend与fts probe进程也有交互，fts probe更新节点状态后，通过ftsProbeInfo-&gt;status_version, backend可以感知到节点状态的变化。而backend在发现有些节点连不上时，也可以通过FtsNotifyProber来通知fts 探测节点是否正常。</p><h1 id="fts-probe"><a href="#fts-probe" class="headerlink" title="fts probe"></a>fts probe</h1><h2 id="fts-loop"><a href="#fts-loop" class="headerlink" title="fts loop"></a>fts loop</h2><p>fts probe 主函数FtsLoop，流程如下：<br><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/FtsLoop.png" alt="FtsLoop"></p><ol><li><p>ftsProbeInfo-&gt;start_count和ftsProbeInfo-&gt;done_count给FtsNotifyProber中使用的。用于等待触发的探测结束。</p></li><li><p>ftsprobe每次探测开始时都会从新从gp_segment_configuration读取节点信息，探测完成后，在销毁。这么做的原因是有些脚本（如gpaddmirrors）可能会改gp_segment_configuration，所以每次都要读取最新的节点信息。</p></li><li><p>每次探测时，如果有节点状态发生变化，则会把gp_segment_configuration的信息dump到gpsegconfig_dump文件，主要的原因是防止在事务外需要访问节点信息。比如在2pc的第二阶段，如果commit or abort失败，需要重试时，就需要读取gpsegconfig_dump文件。**<img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/gpsegconfig_dump.png" alt="gpsegconfig_dump">**</p></li><li><p>ftsProbeInfo-&gt;status_version 表示节点状态信息的版本，每次发生变化都会+1，主要作用是通知其他进程节点状态发生变化了，需要重新更新节点状态，比如主备发生了切换等。ftsProbeInfo-&gt;status_version只会在ftsprobe 进程修改，其他进程都是只读，没有加ftsProbeInfo-&gt;lock。</p></li><li><p>FtsProbeInfo结构信息如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">FtsProbeInfo</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="comment">/* 节点状态信息的版本，每次如果有节点状态信息发生改变，则+1*/</span></span><br><span class="line"><span class="keyword">volatile</span> uint8status_version;</span><br><span class="line">    <span class="comment">/* 记录节点是否down，用于快速判断节点是否活着 */</span></span><br><span class="line"><span class="keyword">volatile</span> uint8status[FTS_MAX_DBS];</span><br><span class="line">    <span class="comment">/* 锁，控制除status之外的field的修改*/</span></span><br><span class="line"><span class="keyword">volatile</span> <span class="keyword">slock_t</span>lock;</span><br><span class="line">    <span class="comment">/* </span></span><br><span class="line"><span class="comment">     * start_count每次探测开始时+1, done_count每次探测与start_count相同</span></span><br><span class="line"><span class="comment">     * 这两个值主要在FtsNotifyProber中使用，用来判断探测是否结束。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">volatile</span> int32start_count;</span><br><span class="line"><span class="keyword">volatile</span> int32done_count;</span><br><span class="line">&#125; FtsProbeInfo;</span><br></pre></td></tr></table></figure><h2 id="FtsWalRepMessageSegments"><a href="#FtsWalRepMessageSegments" class="headerlink" title="FtsWalRepMessageSegments"></a>FtsWalRepMessageSegments</h2><p>master 发送探测消息，处理探测结果，并完成主备切换，流程如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FtsWalRepInitProbeContext</span><br><span class="line">InitPollFds</span><br><span class="line"><span class="keyword">while</span> (!allDone(&amp;context) &amp;&amp; FtsIsActive())</span><br><span class="line">&#123;</span><br><span class="line">    ftsConnect(&amp;context);</span><br><span class="line">    ftsPoll(&amp;context);</span><br><span class="line">    ftsSend(&amp;context);</span><br><span class="line">    ftsReceive(&amp;context);</span><br><span class="line">    processRetry(&amp;context);</span><br><span class="line">    is_updated |= processResponse(&amp;context);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!FtsIsActive())</span><br><span class="line">释放连接和资源</span><br><span class="line">resetMarkPrimaryDead</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">int</span> num_pairs; <span class="comment">/* number of primary-mirror pairs FTS wants to probe */</span></span><br><span class="line">fts_segment_info *perSegInfos;</span><br><span class="line">&#125; fts_context;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">CdbComponentDatabaseInfo *primary_cdbinfo;</span><br><span class="line">CdbComponentDatabaseInfo *mirror_cdbinfo;</span><br><span class="line">fts_result result;</span><br><span class="line">FtsMessageState state;</span><br><span class="line"><span class="keyword">short</span> poll_events;</span><br><span class="line"><span class="keyword">short</span> poll_revents;</span><br><span class="line">int16 fd_index;               <span class="comment">/* index into PollFds array */</span></span><br><span class="line"><span class="keyword">pg_time_t</span> startTime;          <span class="comment">/* probe start timestamp */</span></span><br><span class="line"><span class="keyword">pg_time_t</span> retryStartTime;     <span class="comment">/* time at which next retry attempt can start */</span></span><br><span class="line">int16 probe_errno;            <span class="comment">/* saved errno from the latest system call */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pg_conn</span> *<span class="title">conn</span>;</span>         <span class="comment">/* libpq connection object */</span></span><br><span class="line"><span class="keyword">int</span> retry_count;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* recover mode */</span></span><br><span class="line">XLogRecPtr xlogrecptr;</span><br><span class="line"><span class="keyword">bool</span> recovery_making_progress;</span><br><span class="line">&#125; fts_segment_info;</span><br></pre></td></tr></table></figure><p>fts context 记录本次探测的所有相关信息，主要记录在fts_segment_info， 每个对segment对应一个fts_segment_info，状态信息主要记录在primary_cdbinfo和mirror_cdbinfo里面，其他的字段都是用来控制探测，已经异常处理的。</p><h2 id="fts-探测过程"><a href="#fts-探测过程" class="headerlink" title="fts 探测过程"></a>fts 探测过程</h2><p>fts的探测过程是通过状态机控制的，根据不同的状态，进行相应的处理，状态机的转换主要在FtsWalRepMessageSegments函数内，状态机是按segment为单位的，每个segment使用单独状态机，互不影响，状态转换如下图所示：<br><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/fts-probe-state-machine.png" alt="fts state machine"></p></li><li><p>FTS_PROBE_SEGMENT为起始状态，每次探测开始所有segment都出于改状态</p></li><li><p>FTS_PROBE_SUCCESS 分为下面几种情况：</p><ol><li>mirror 探测失败：如果要求重试(由参数gp_fts_mark_mirror_down_grace_period控制)，则忽略本次探测结果，重新探测；否则标记mirror down，并把状态改为FTS_SYNCREP_OFF_SEGMENT，通知primary 关闭同步复制。</li><li>primary和mirror同步状态发送改变: 同步状态从sync –&gt; not sync 或者not sync –&gt;sync</li><li>探测到mirror: 探测信息发送到mirror上了。正常情况下探测消息只会发给primary，这种情况一般是之前已经做完主备切换，但是mirror还没有完成切换。此时把状态改成FTS_PROMOTE_SEGMENT,再次通知mirror升主。</li></ol></li><li><p>FTS_PROBE_FAILED分为下面几种情况：</p><ol><li>需要重试(由参数gp_fts_probe_retries控制)，这修改状态为：FTS_PROBE_RETRY_WAIT</li><li>重试gp_fts_probe_retries次后，依然失败：<ol><li>primary 处于 recover mode，说明primary 正在恢复自身异常，过一会可能就会正常了，所有忽略本次探测结果。</li><li>切主。把处于sync状态的mirror 生成主。主要是更新catalog状态，然后把状态改成FTS_PROMOTE_SEGMENT</li><li>没有处于sync的mirror，无法自动恢复，需要人工介入：比如想办法重新恢复primary或者强制切换。</li></ol></li></ol></li><li><p>FTS_SYNCREP_OFF_SEGMENT：mirror down了，通知primary 关闭同步复制，避免查询hang住</p><ol><li>FTS_SYNCREP_OFF_SUCCESS：关闭成功，结束本次探测</li><li>FTS_SYNCREP_OFF_FAILED：关闭失败，重试gp_fts_probe_retries次。如果依然失败，则结束本次探测。</li></ol></li><li><p>FTS_PROMOTE_SEGMENT： primary down，通知mirror升级为primary</p><ol><li>FTS_PROMOTE_SUCCESS：升主成功，结束本次探测</li><li>FTS_PROMOTE_FAILED：升主失败，重试gp_fts_probe_retries次，如果依然失败，则结束本次探测。此时会影响数据库的使用，需要人工介入恢复。</li></ol></li><li><p>FTS_RESPONSE_PROCESSED：探测结束状态</p><h2 id="fts-消息格式"><a href="#fts-消息格式" class="headerlink" title="fts 消息格式"></a>fts 消息格式</h2><p>fts 探测消息格式固定，如下：<br>message type | primary_dbid | segment_contentid</p></li></ol><p>其中message type分别是：<br>FTS_PROBE_SEGMENT、FTS_SYNCREP_OFF_SEGMENT、FTS_PROMOTE_SEGMENT</p><h2 id="相关参数"><a href="#相关参数" class="headerlink" title="相关参数"></a>相关参数</h2><p>fts相关的几个参数，以及默认值：<br>gp_fts_probe_retries：3<br>gp_fts_probe_interval: 30s<br>gp_request_fts_probe_scan<br>gp_fts_probe_timeout: 60s<br>gp_fts_mark_mirror_down_grace_period:  mirror 失联多久才标记为down<br><strong>最大切换时间: 30 + 3 * (60 + 1) = 213s**<br>*<em>如果不发生timeout是: 30 + 3</em>1 = 33s</strong><br>**</p><h1 id="fts-message-handler"><a href="#fts-message-handler" class="headerlink" title="fts message handler"></a>fts message handler</h1><p>fts message handler 进程是在primary segment上处理master probe 消息的进程，该进程不是常驻进程，每次轮探测结束后，都会退出。<br>入口函数是HandleFtsMessage，主要处理master发送的3中消息，分别是：</p><ul><li><input checked disabled type="checkbox"> <strong>HandleFtsWalRepProbe</strong></li></ul><p>处理FTS_PROBE_SEGMENT消息，主要是通过wal sender查看mirror是否活着，并且检测primary 磁盘是否可读可写。</p><ol><li>mirror的处理：跟进WalSnd记录的信息，判断mirror是否活着以及是否与primary 同步</li><li>如果primary 与mirror失联了，则根据gp_fts_mark_mirror_down_grace_period的值，确定是否需要master标记为down或者重试。</li><li>如果mirror活着，并且处于非同步复制状态，则修改为同步复制</li></ol><ul><li><input checked disabled type="checkbox"> <strong>HandleFtsWalRepPromote</strong></li></ul><p>处理FTS_PROMOTE_SEGMENT消息，升级当前的mirror为primary：</p><ol><li>UnsetSyncStandbysDefined，修改为非同步复制，避免查询hang住</li><li>CreateReplicationSlotOnPromote，创建新的replication slots，避免xlog被回收。</li><li>SignalPromote，通知pm进程promote为主</li></ol><ul><li><input checked disabled type="checkbox"> <strong>HandleFtsWalRepSyncRepOff</strong></li></ul><p>处理FTS_SYNCREP_OFF_SEGMENT消息，关闭同步复制：</p><ol><li>UnsetSyncStandbysDefined</li><li>GetMirrorStatus</li></ol><h1 id="backend-与fts-probe的交互"><a href="#backend-与fts-probe的交互" class="headerlink" title="backend 与fts probe的交互"></a>backend 与fts probe的交互</h1><ul><li><input checked disabled type="checkbox"> <strong>交互的方式有哪些？</strong></li></ul><ol><li><p><strong>ftsProbeInfo-&gt;status_version</strong>: fts在每次改过segment信息后，会增加ftsProbeInfo-&gt;status_version，backend可以通过ftsProbeInfo-&gt;status_version比较自己缓存的版本是否过期，如果过期，则从新从catalog reload segment信息.</p></li><li><p>FtsNotifyProber: 其他进程可以调用该函数通知fts进行新一轮的探测</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FtsNotifyProber</span><br><span class="line">readGpSegConfigFromFTSFiles</span><br><span class="line">cdbcomponent_updateCdbComponents</span><br><span class="line">cdbcomponent_getCdbComponents</span><br><span class="line">checkDispatchResult</span><br><span class="line">cdbgang_createGang_async</span><br><span class="line">gp_request_fts_probe_scan</span><br></pre></td></tr></table></figure><p>backend 在某些阶段发现节点通信异常时，会通知fts进行一次探测，看看节点是否正常。</p></li><li><p>SendPostmasterSignal(PMSIGNAL_WAKEN_FTS);</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SendPostmasterSignal(PMSIGNAL_WAKEN_FTS)</span><br><span class="line">cdbCopyEndInternal</span><br><span class="line">FtsNotifyProber</span><br><span class="line">sigusr1_handler</span><br><span class="line">PostmasterMain</span><br></pre></td></tr></table></figure><p>改方式与FtsNotifyProber的区别是，只是唤醒fts，做一次探测，但是backend并不会等待探测完成。</p></li></ol><ul><li><input checked disabled type="checkbox"> <strong>backend 什么时候能够收到fts 的信息变更？</strong></li></ul><ol><li>在事务开始时, StartTransaction会调用cdbcomponent_updateCdbComponents，比较版本是否过期，如果过期，则更新缓存。</li><li>doNotifyingCommitNotPrepared、doNotifyingCommitPrepared、retryAbortPrepared</li></ol><p>在上面3个事务处理函数中失败时，再次重试的时候，会从新更新缓存</p><ol start="3"><li>doNotifyingAbort、rollbackDtxTransaction</li></ol><p>在处理NO_PREPARED的事务时，如果失败，则会清除掉缓存cdb_component_dbs，后面再有操作时，就会更新缓存</p><ol start="4"><li>AtAbort_DispatcherState</li></ol><p>在writer gang出错时，会清除掉换成cdb_component_dbs，后面再有操作时，就会更新缓存</p><ol start="5"><li>assign_gp_role</li></ol><p>更新backend角色时，可能会更新缓存。</p><p>从上面可以看到，backend更新新的节点信息，主要在事务启动，或者事务结束时发生异常后，而在事务运行过程中，是不会更新节点信息的，这也是正确的，因为按现有的架构，在事务中间出错后，是无法继续做的，必须rollback。</p><ul><li><input checked disabled type="checkbox"> <strong>会不会出现双写的情况？</strong></li></ul><p>会不会出现有些事务在旧的primary上，而有些事务在新的primary上？只有提交的事务才会有问题，未提交的没有问题？出现双写的必要条件是master 认为primary down了，但是mirror还活着并且是sync状态。<br>考虑下面的场景：</p><ol><li><strong>网络抖动，导致fts认为primary挂掉了，但是实际还活着，则已经启动的事务还是会往旧primary 发送sql，而新的事务会在新的primary上。</strong></li></ol><p>场景构造：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-- 先修改gp_fts_probe_interval&#x3D;20s，gp_fts_probe_timeout&#x3D;20s, gp_fts_probe_retries &#x3D; 1</span><br><span class="line"></span><br><span class="line">--场景1：</span><br><span class="line">begin;</span><br><span class="line"></span><br><span class="line">-- 使用iptables 模拟一个primary segment网络故障</span><br><span class="line">-- 等fts 检测到故障并切换,然后恢复网络故障</span><br><span class="line"></span><br><span class="line">insert into test values(20),(21),(22);</span><br><span class="line">commit;</span><br><span class="line"></span><br><span class="line">--场景2：</span><br><span class="line">-- 恢复集群到初始状态</span><br><span class="line">begin;</span><br><span class="line">insert into test values(30),(31),(32);</span><br><span class="line"></span><br><span class="line">-- 使用iptables 模拟一个primary segment网络故障</span><br><span class="line">-- 等fts 检测到故障并切换后</span><br><span class="line">commit;</span><br></pre></td></tr></table></figure><p>查看事务是否能正常提交？？<br>场景1：<br><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/case1.png" alt="image.png"><br>这个是为什么？？？<br>场景2：</p><p><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/case2.png" alt="image.png"><br>不过master端还是发现了切换：<br><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/case2-1.png" alt="image.png"><br>这个是因为checkDispatchResult在poll时，发现超时了，然后调用了checkSegmentAlive，看看节点是否有问题，发现该节点已经发生了切换，所以就报出了上面的错误。poll超时的原因是旧的primary在等待commit xlog同步到mirror，但是mirror已经不存在了(promote为新的primary了)。</p><p>这个是通过强同步来保证没有问题的，所以如果primary和mirror不是强同步的话，则就有问题了。所以可以得出一个结论：<strong>在非强同步时，会出现双写问题。</strong><br><strong>不过我试了一下，数据确实会不一致，但不是因为双写造成的，还不确定到底是什么原因。旧primary在提交时出现下面的错误：</strong><br><strong><img src="/2020/12/28/Greenplum-Fts%E5%AE%9E%E7%8E%B0/case2-2.png" alt="image.png"></strong><br><strong>上面的错误是重试之后的错误，在提交时为什么会失败？原因是在prepared commit之后会回收gang，在回收时会检查对应的segmeng是否down了，如果down了，则不回收gang。</strong><br><strong>所以要制造这个问题需要在执行prepared commit之后，hang住master，然后在制造网络故障。</strong><br>**</p><ol start="2"><li><strong>sync_standbys_defined的维护是有时间窗口的，如果在这个时间窗口内出现上面的问题呢？</strong></li></ol><p>有哪些情况sync_standbys_defined是null的？<br>没有mirror<br>fts认为mirror down了，primary 关闭sync_standbys_defined<br>mirror promote之前，关闭sync_standbys_defined</p><p><strong>【备注】</strong><br><strong>Q: 什么时候会走doNotifyingCommitNotPrepared？</strong><br>A: 显示的只读事务，优化了两阶段提交，比如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">begin</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> test;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure><p><strong>Q: sync_standbys_defined 与 SyncCommitLevel的关系？？</strong><br>**A: **sync_standbys_defined 表示是否有强同步的节点在，如果不为空的话，则表示数据一定要同步到mirror上才能结束事务，不论是否有活的mirror存在。SyncCommitLevel则表示同步级别。表示完成了哪些级别就任务是同步结束了。如果sync_standbys_defined 为false，则即使把SyncCommitLevel配置成最高级别也没有用。commit也会立即返回，不会等待同步到mirror上。具体参考SyncRepWaitForLSN函数代码。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Greenplum 总共存在2个角色，每个角色有分别实现了standby，分别是：master和master的standby， primary segment和mirror segment。数据库的fault tolerance由fault tolerance server（简称FTS）来处理。&lt;br&gt;FTS主要监控和控制segment节点的状态，以及发生异常时进行切换，FTS并不监控standby的状态，目前的实现是由管控控制。&lt;br&gt;    FTS 由2类进程组成：ftsprobe 进程和 fts handle message 进程，ftsprobe 进程在master上，定期给segment发送探活消息， fts handle message 进程是segment上处理ftsprobe 进程探活消息的，改进程不是守护进程，处理完一次探活后，即退出。&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="ha" scheme="https://zisedeqing.github.io/categories/greenplum6-0/ha/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="fts" scheme="https://zisedeqing.github.io/tags/fts/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum shared snapshot</title>
    <link href="https://zisedeqing.github.io/2020/12/28/Greenplum-shared-snapshot/"/>
    <id>https://zisedeqing.github.io/2020/12/28/Greenplum-shared-snapshot/</id>
    <published>2020-12-28T13:33:03.000Z</published>
    <updated>2020-12-29T01:32:53.309Z</updated>
    
    <content type="html"><![CDATA[<p>shared snapshot是gp新引入的一个机制，用于在每个segment上同一个transaction的QE writer与QE reader之间共享snapshot。</p><a id="more"></a><h1 id="引入的原因"><a href="#引入的原因" class="headerlink" title="引入的原因"></a>引入的原因</h1><p>gp为了提升查询性能，把一个plan会查分成多个slice，每个slice在segment上就是一个可以独立运行的执行单元，多个slice使用树形结构组织，数据从下面的slice流向上面的slice，并行执行。每个slice实际上就对应于segment上的一个backend进程(gp称为QE)， 在gp中有两种QE，QE writer和QE reader (分别对应于writer gang和reader gang)。在segment上transaction的所有数据写入操作只能由QE writer来做，但是QE writer和QE reader又属于同一个transaction，所以QE writer和QE reader必须使用相同的snapshot，才能保证数据的一致性。QE writer和QE reader在segment上又是单独backend，所以对于pg来说，他们应该是2个不同的transaction，看到的数据也是不一样的。所以gp就引入了shared snapshot来解决这个问题。</p><h1 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h1><p>QE writer把snapshot 写入共享内存中，QE writer在需要snapshot时，根据mpp_session_id在共享内存中查找QE writer生成的snapshot，这样就使用了与QE writer相同的snapshot，对于数据的可见性也保证了。</p><h1 id="share了哪些"><a href="#share了哪些" class="headerlink" title="share了哪些"></a>share了哪些</h1><p>snapshot<br>sub-transaction status<br>combo-cid mapping<br>等</p><h1 id="关于cursor的处理"><a href="#关于cursor的处理" class="headerlink" title="关于cursor的处理"></a>关于cursor的处理</h1><p>shared snapshot在设计的初衷是满足一般sql的，即执行时使用的是current snapshot，但是cursor使用的snapshot是cursor创建时的snapshot，不是current snapshot。并且cursor还有一个不同的地方是，对于同一个transaction是可以由多个cursor同时存在的，这样就需要存储多个snapshot。<br>gp解决这个问题的方式是，把current snapshot 写入临时文件，QE reader使用时，从临时文件读取对应的snapshot，而不是使用共享内存中的shared snapshot。<br>细节可以看一下：<a href="https://github.com/greenplum-db/gpdb/blob/master/src/backend/utils/time/sharedsnapshot.c">https://github.com/greenplum-db/gpdb/blob/master/src/backend/utils/time/sharedsnapshot.c</a> 文件头注释。</p><h1 id="相关代码"><a href="#相关代码" class="headerlink" title="相关代码"></a>相关代码</h1><p>我们使用的gp版本（4.3.99）shared snapshot代码在tqual.c里面的，后面gp把这块代码单独领出来了。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">SharedSnapshotStruct</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">int</span> numSlots;<span class="comment">/* number of valid Snapshot entries */</span></span><br><span class="line"><span class="keyword">int</span>maxSlots;<span class="comment">/* allocated size of sharedSnapshotArray */</span></span><br><span class="line"><span class="keyword">int</span> nextSlot;<span class="comment">/* points to the next avail slot. */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We now allow direct indexing into this array.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * We allocate the XIPS below.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Be very careful when accessing fields inside here.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">SharedSnapshotSlot   *slots;</span><br><span class="line"></span><br><span class="line">TransactionId   *xips;<span class="comment">/* VARIABLE LENGTH ARRAY */</span></span><br><span class="line">&#125; SharedSnapshotStruct;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">volatile</span> SharedSnapshotStruct *sharedSnapshotArray;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">SharedSnapshotSlot</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">int4slotindex;  <span class="comment">/* where in the array this one is. */</span></span><br><span class="line">int4 slotid;</span><br><span class="line"><span class="keyword">pid_t</span> pid; <span class="comment">/* pid of writer seg */</span></span><br><span class="line">TransactionIdxid;</span><br><span class="line">CommandId       cid;</span><br><span class="line">TimestampTzstartTimestamp;</span><br><span class="line"><span class="keyword">volatile</span> TransactionId   QDxid;</span><br><span class="line"><span class="keyword">volatile</span> CommandIdQDcid;</span><br><span class="line"><span class="keyword">volatile</span> <span class="keyword">bool</span>ready;</span><br><span class="line"><span class="keyword">volatile</span> uint32segmateSync;</span><br><span class="line">uint32total_subcnt;<span class="comment">/* Total # of subxids */</span></span><br><span class="line">uint32inmemory_subcnt;    <span class="comment">/* subxids in memory */</span></span><br><span class="line">TransactionId   subxids[MaxGpSavePoints];</span><br><span class="line">uint32combocidcnt;</span><br><span class="line">ComboCidKeyData combocids[MaxComboCids];</span><br><span class="line">SnapshotDatasnapshot;</span><br><span class="line"></span><br><span class="line">&#125; SharedSnapshotSlot;</span><br><span class="line"></span><br><span class="line"><span class="keyword">volatile</span> SharedSnapshotSlot *SharedLocalSnapshotSlot = <span class="literal">NULL</span>;</span><br></pre></td></tr></table></figure><p>CreateSharedSnapshotArray:<br>创建并初始化sharedSnapshotArray<br><img src="/2020/12/28/Greenplum-shared-snapshot/CreateSharedSnapshotArray.png" alt="CreateSharedSnapshotArray"><br>addSharedSnapshot：<br>初始化当前session的shared snapshot并插入到sharedSnapshotArray。<br>初始化SharedLocalSnapshotSlot<br>role: GP_ROLE_DISPATCH || (GP_ROLE_EXECUTE &amp;&amp; Gp_is_writer)</p><p><img src="/2020/12/28/Greenplum-shared-snapshot/addSharedSnapshot.png" alt="addSharedSnapshot"></p><p>lookupSharedSnapshot:<br>从sharedSnapshotArray中找到当前session对应的shared snapshot,并赋给SharedLocalSnapshotSlot<br>role: Entry db singleton QE || (GP_ROLE_EXECUTE &amp;&amp; !Gp_is_writer)<br><img src="/2020/12/28/Greenplum-shared-snapshot/lookupSharedSnapshot.png" alt="lookupSharedSnapshot"></p><p>SharedSnapshotRemove：<br>回收当前session占用的shared snapshot slot 给sharedSnapshotArray<br><img src="/2020/12/28/Greenplum-shared-snapshot/SharedSnapshotRemove.png" alt="SharedSnapshotRemove"></p><p>dumpSharedLocalSnapshot_forCursor<br>把SharedLocalSnapshotSlot dump到文件<br>只有在extended query (declare cusor/PBE protocol)时才会走该流程，由master调用verify_shared_snapshot_ready让所有的节点执行dump。<br>    <img src="/2020/12/28/Greenplum-shared-snapshot/dumpSharedLocalSnapshot_forCursor.png" alt="dumpSharedLocalSnapshot_forCursor"><br>readSharedLocalSnapshot_forCursor<br>从dump的文件恢复shared snapshot，只有extended query时才会走该流程<br>    <img src="/2020/12/28/Greenplum-shared-snapshot/readSharedLocalSnapshot_forCursor.png" alt="readSharedLocalSnapshot_forCursor"></p><p>updateSharedLocalSnapshot<br>更新SharedLocalSnapshotSlot<br>    <img src="/2020/12/28/Greenplum-shared-snapshot/updateSharedLocalSnapshot.png" alt="updateSharedLocalSnapshot"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;shared snapshot是gp新引入的一个机制，用于在每个segment上同一个transaction的QE writer与QE reader之间共享snapshot。&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum4.3" scheme="https://zisedeqing.github.io/categories/greenplum4-3/"/>
    
    <category term="transaction" scheme="https://zisedeqing.github.io/categories/greenplum4-3/transaction/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="snapshot" scheme="https://zisedeqing.github.io/tags/snapshot/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum 列存原理分析</title>
    <link href="https://zisedeqing.github.io/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"/>
    <id>https://zisedeqing.github.io/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/</id>
    <published>2020-12-28T13:15:43.000Z</published>
    <updated>2020-12-29T01:32:53.309Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-相关代码"><a href="#1-相关代码" class="headerlink" title="1. 相关代码"></a>1. 相关代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">src&#x2F;backend&#x2F;access&#x2F;aocs</span><br><span class="line">                    aocs_compaction.c</span><br><span class="line">                    aocsam.c</span><br><span class="line">                    aocssegfiles.c</span><br><span class="line">                &#x2F;appendonly</span><br><span class="line">                    appendonlyam.c</span><br><span class="line">                    aomd.c</span><br><span class="line">                    aosegfiles.c</span><br><span class="line">                    appendonly_compaction.c</span><br><span class="line">                    appendonly_visimap.c</span><br><span class="line">                    appendonly_visimap_entry.c</span><br><span class="line">                    appendonly_visimap_store.c</span><br><span class="line">                    appendonly_visimap_udf.c</span><br><span class="line">                    appendonlyblockdirectory.c</span><br><span class="line">                    appendonlytid.c</span><br><span class="line">                    appendonlywriter.c</span><br><span class="line">src&#x2F;backend&#x2F;utils&#x2F;datumstream</span><br><span class="line">                    datumstream.c</span><br><span class="line">                    datumstreamblock.c</span><br><span class="line">src&#x2F;backend&#x2F;cdb&#x2F;</span><br><span class="line">                cdbbufferedappend.c</span><br><span class="line">                cdbbufferedread.c</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-组织结构"><a href="#2-组织结构" class="headerlink" title="2. 组织结构"></a>2. 组织结构</h1><p><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/aocs-%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="aocs-组织结构图"></p><h2 id="2-1-segfile"><a href="#2-1-segfile" class="headerlink" title="2.1 segfile"></a>2.1 segfile</h2><ol><li>不同的列存储在不同的文件内，</li><li>一个列的物理文件最多只有128个</li><li>列文件以block为单位组织，压缩后的block大小不一，并且该文件没有文件头信息。</li><li>压缩之前的block大小也不是固定的，根据当前事务的写入量而定。</li><li>segfile size没有限制，只限制一个segfile最多存储的行数：2^40-1 (约10095亿)。并且默认情况下在存储到达90%时，会切换新的文件(具体细节查看SetSegnoForWrite函数) </li></ol><p>gp的segfile存在溢出bug，gp在insert to segfile时并不会检查rowcount是否超过2^40-1，所以如果再同一个事务内插入大量数据到一个将要满的segfile，就可能会导致文件溢出。而这个溢出操作并不会导致数据库报错，而是rownum &gt; 2^40-1的数据都会丢失。</p><h2 id="2-2-rownum"><a href="#2-2-rownum" class="headerlink" title="2.2 rownum"></a>2.2 rownum</h2><h3 id="2-2-1-为何引入rownum"><a href="#2-2-1-为何引入rownum" class="headerlink" title="2.2.1 为何引入rownum"></a>2.2.1 为何引入rownum</h3><p>对于heap表，pg使用tid表示tuple的物理位置，但是对于aocs表，由于压缩的原因，无法确定一个tuple在文件中的物理位置，所以gp引入了rownum表示tuple在文件中的逻辑位置。</p><ol><li>rownum 是递增的，但可能不连续，不连续的原因是insert时为了效率每次都是生成一批rownum</li><li>每个tuple都唯一的对应一个rownum，与tid一样</li><li>不同的segfile的tuple rownum单独计算</li></ol><p>所以在同一个block内，rownum一定是连续的</p><ol start="4"><li>如何使用rownum找到对应的tuple</li></ol><p>首先block header记录的当前block的第一个rownum和rowcount，所以只需要遍历整个文件，如果rownum 在[firstrownum, firstrownum+rowcount]范围内，则找到tuple对应的 block，遍历block即可。</p><h3 id="2-2-2-aocs的tuple-id"><a href="#2-2-2-aocs的tuple-id" class="headerlink" title="2.2.2 aocs的tuple id"></a>2.2.2 aocs的tuple id</h3><p>gp使用AOTupleId 表示tuple id(与pg的tid对应，heap表使用ItemPointer表示tid)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">typedef struct AOTupleId</span><br><span class="line">&#123;</span><br><span class="line">uint16      bytes_0_1;</span><br><span class="line">uint16bytes_2_3;</span><br><span class="line">uint16bytes_4_5;</span><br><span class="line"></span><br><span class="line">&#125; AOTupleId;</span><br></pre></td></tr></table></figure><p>最左7位是segment no，所以最多只有128文件<br>最右的第16位是保留位，一定是1<br>剩下的40位元组在文件的位置标记，即rownum。<br>如下图所示：<br><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/rownum.png" alt="rownum"></p><ul><li><input checked disabled type="checkbox"> <strong>为什么第33位要标记为1？</strong></li></ul><p>主要原因是AOTupleId在一些地方如executor等会转换成行存的Itempointer，在代码中有很多ItemPointerIsValid的判断，为了简单gp把第33位标记为1.<br>实际上要过ItemPointerIsValid的判断，只需要把最后16位的任意一位标记为1即可，不确定为什么选择最后16位的第一个位。</p><h2 id="2-3-block"><a href="#2-3-block" class="headerlink" title="2.3 block"></a>2.3 block</h2><p>对于列存的block，每个block有2个header，一个是block header，另一个是datum header。其中block header是不会压缩的，datum header会压缩。</p><h3 id="2-3-1-header"><a href="#2-3-1-header" class="headerlink" title="2.3.1 header"></a>2.3.1 header</h3><ul><li><input checked disabled type="checkbox"> <strong>block header</strong></li></ul><p>block header size： 8bytes + 8bytes + 4 bytes = 20 bytes<br><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/block-header.png" alt="block header"><br>类型：</p><ol><li>AoHeaderKind_SmallContent  一般的<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">AOSmallContentHeader</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">      uint32            smallcontent_bytes_0_3;</span><br><span class="line">      uint32            smallcontent_bytes_4_7;</span><br><span class="line">&#125;AOSmallContentHeader</span><br></pre></td></tr></table></figure></li><li>AoHeaderKind_LargeContent 超长字段，一个block存不下，分成多个block存储</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">typedef struct AOLargeContentHeader</span><br><span class="line">&#123;</span><br><span class="line">    uint32            largecontent_bytes_0_3;</span><br><span class="line">    uint32            largecontent_bytes_4_7;</span><br><span class="line">&#125;AOLargeContentHeader</span><br></pre></td></tr></table></figure><p>如果一个字段长度超长，则会被分割成多个block存储，方式是：<br>large block|small block|…|small block</p><ol start="3"><li>AoHeaderKind_NonBulkDenseContent </li></ol><p>column compress type = rle_type<br>column compress level = 1 &amp;&amp;<br>rowcount &gt; 16383</p><ol start="4"><li>AoHeaderKind_BulkDenseContent</li></ol><p>column compress type = rle_type<br>column compress level &gt; 1 &amp;&amp;<br>rowcount &gt; 16383<br><strong>未看到哪种情况会走这个</strong></p><p>[x] <strong>datum header</strong><br>类型：</p><ol><li>DatumStreamVersion_Original</li></ol><p>无压缩或者zlib压缩</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Datum Stream Block (Original).</span></span><br><span class="line"><span class="comment"> * 16 bytes header.  Followed by data.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DatumStreamBlock_Orig</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">int16version;<span class="comment">/* version number */</span></span><br><span class="line">int16flags;<span class="comment">/* some flags */</span></span><br><span class="line">int16ndatum;<span class="comment">/* number of datum, including null */</span></span><br><span class="line">int16unused;<span class="comment">/* unused */</span></span><br><span class="line">int32nullsz;<span class="comment">/* size nullbitmaps */</span></span><br><span class="line">int32sz;<span class="comment">/* logical data size, not including header,</span></span><br><span class="line"><span class="comment"> * nullbitmap, and padding */</span></span><br><span class="line">&#125;DatumStreamBlock_Orig;</span><br></pre></td></tr></table></figure><ol start="2"><li>DatumStreamVersion_Dense</li></ol><p>目前版本无该类型的block，猜测是用于quicklz压缩的</p><ol start="3"><li>DatumStreamVersion_Dense_Enhanced</li></ol><p>压缩类型为: RLE_TYPE<br>DatumStreamBlock_Dense<br>DatumStreamBlock_Rle_Extension<br>DatumStreamBlock_Delta_Extension</p><h3 id="2-3-2-block-size"><a href="#2-3-2-block-size" class="headerlink" title="2.3.2 block size"></a>2.3.2 block size</h3><p>列存block size 范围：[8KB, 2MB]，压缩之前的size<br>默认size是：32KB<br>真实的block size受限于用户建表时定义的blocksize，如果没有指定blocksize，则使用默认的blocksize。<br>还有一个参数会影响block size，就是gp列存限定了一个block最多可以存多少个value，如果超过这个值，即使没有达到block size限制还是会写入到另外的block内的。</p><ol><li>DatumStreamVersion_Original</li></ol><p>最多16383行, 即small content header row count最大值：2^14 - 1</p><ol start="2"><li>DatumStreamVersion_Dense/DatumStreamVersion_Dense_Enhanced</li></ol><p>最多2^30 -1行, 即noblukdense header row count最大值</p><h3 id="2-3-3-null-bitmap"><a href="#2-3-3-null-bitmap" class="headerlink" title="2.3.3 null bitmap"></a>2.3.3 null bitmap</h3><p>用来存储null值</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DatumStreamBitMapWrite</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">uint8   *buffer;</span><br><span class="line">int32bufferSize;</span><br><span class="line"></span><br><span class="line">uint8byteBit;</span><br><span class="line">uint8   *bytePointer;</span><br><span class="line">int32bitOnCount;</span><br><span class="line">int32bitCount;</span><br><span class="line">&#125;DatumStreamBitMapWrite;</span><br></pre></td></tr></table></figure><h1 id="3-辅助元信息表"><a href="#3-辅助元信息表" class="headerlink" title="3. 辅助元信息表"></a>3. 辅助元信息表</h1><h2 id="3-1-库级别的meta表"><a href="#3-1-库级别的meta表" class="headerlink" title="3.1 库级别的meta表"></a>3.1 库级别的meta表</h2><p>pg_appendonly gp原有的，记录所有的appendonly表<br>gp_fastsequence rownum使用的sequence，segment上的有用</p><h2 id="3-2-表级别的meta表"><a href="#3-2-表级别的meta表" class="headerlink" title="3.2 表级别的meta表"></a>3.2 表级别的meta表</h2><p>下面的meta表是对于每个ao表都有，如果表是分区表，则每个自分区也都会有下面这些meta表。</p><h3 id="3-2-1-查询方法"><a href="#3-2-1-查询方法" class="headerlink" title="3.2.1 查询方法"></a>3.2.1 查询方法</h3><p>SELECT gp_segment_id, * from gp_dist_random(‘pg_aoseg.pg_aoblkdir_17684’)<br>注意使用utility直接登录segment查询是不行的，估计是可见性问题吧</p><h3 id="3-2-2-pg-aocsseg-XXX"><a href="#3-2-2-pg-aocsseg-XXX" class="headerlink" title="3.2.2 pg_aocsseg_XXX"></a>3.2.2 pg_aocsseg_XXX</h3><p>记录segfile 相关信息，包含元组数量、文件长度、修改次数等。</p><table><thead><tr><th>column</th><th>type</th><th>desc</th></tr></thead><tbody><tr><td>segno</td><td>integer</td><td>文件号，表示第几个文件。对应于真实的文件是:</td></tr><tr><td>relfilenode.segno+（column_number - 1）* 128</td><td></td><td></td></tr><tr><td>比如relfilenode=451012的第二列的segno=2的文件名是：</td><td></td><td></td></tr><tr><td>451012.130</td><td></td><td></td></tr><tr><td>tupcount</td><td>bigint</td><td>元组数量，包含已经删除的和更新的。如果这个值与select count(*)差距比较大的话，就说明垃圾数据较多，需要做vacuum</td></tr><tr><td>varblockcount</td><td>bigint</td><td></td></tr><tr><td>vpinfo</td><td>bytea</td><td>记录文件长度，包含真实的eof和未压缩后的eof</td></tr><tr><td>modcount</td><td>bigint</td><td>文件修改次数</td></tr><tr><td>state</td><td>smallint</td><td>文件状态：USECURRENT/DEFAULT/AWAITING_DROP</td></tr></tbody></table><p>一个例子：<br><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/pg_aocsseg_XXX.png" alt="pg_aocsseg_XXX"></p><h3 id="3-2-3-pg-aovisimap-XXX"><a href="#3-2-3-pg-aovisimap-XXX" class="headerlink" title="3.2.3 pg_aovisimap_XXX"></a>3.2.3 pg_aovisimap_XXX</h3><p>记录segfile里面的元组可见性信息，主要作用于update/delete，对于rollback导致的不可见是由pg_aocsseg_xxx里面记录的文件长度保证的。</p><table><thead><tr><th>column</th><th>type</th><th>desc</th></tr></thead><tbody><tr><td>segno</td><td>integer</td><td>segfile number</td></tr><tr><td>first_row_no</td><td>bigint</td><td>当前范围内的第一个row num，从0开始，每个元组记录32768个元组可见性信息。</td></tr><tr><td>visimap</td><td>bit varying</td><td>记录元组可见性的bit map</td></tr></tbody></table><p>例子：<br><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/pg_aovisimap_XXX.png" alt="pg_aovisimap_XXX"></p><p>如何判断一个rownum是否在当前visiMapEntry内：<br>bitoffset = rownum - first_row_no<br>(rownum &gt;= first_row_no &amp;&amp; rownum &lt; first_row_no + 32768) &amp;&amp;  (visimap &amp; (1&lt;&lt;bitoffset) != 0) == true ?</p><h3 id="3-2-4-pg-aoblkdir-XXX"><a href="#3-2-4-pg-aoblkdir-XXX" class="headerlink" title="3.2.4 pg_aoblkdir_XXX"></a>3.2.4 pg_aoblkdir_XXX</h3><p>块字典meta表，用于记录block位置信息。创建表的时候，默认不创建该表，在创建表的第一个index时，会创建该meta表。由于ao表的AOTupleId只记录的tuple的逻辑位置信息，不像heap的tid记录的是物理位置信息，在使用index scan时，无法通过AOTupleId直接的找到tuple，所以gp添加了pg_aoblkdir_XXX表，记录block的物理位置信息，来加上index scan tuple的fetch。<br>该系统表上以一个索引，索引列是(segno, columngroup_no, first_row_no)。</p><table><thead><tr><th>column</th><th>type</th><th>desc</th></tr></thead><tbody><tr><td>segno</td><td>integer</td><td>segfile number</td></tr><tr><td>columngroup_no</td><td>integer</td><td>column number</td></tr><tr><td>first_row_no</td><td>bigint</td><td>该行覆盖的第一个tuple的rownum</td></tr><tr><td>minipage</td><td>bit varying</td><td>minEntry的数组，每个entry覆盖多个block</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">typedef struct MinipageEntry</span><br><span class="line">&#123;</span><br><span class="line">int64 firstRowNum;   &#x2F;&#x2F; 当前entry覆盖的第一个tuple的rownum</span><br><span class="line">int64 fileOffset;    &#x2F;&#x2F; 当前entry 覆盖的第一个block在文件中的偏移，该偏移是开始位置的偏移</span><br><span class="line">int64 rowCount;      &#x2F;&#x2F; 当前entry覆盖的所有block的tuple个数 </span><br><span class="line">   &#x2F;&#x2F; (可能会比实际的个数要大，因为有些情况是计算出来的)</span><br><span class="line">&#125; MinipageEntry;</span><br><span class="line"></span><br><span class="line">typedef struct Minipage</span><br><span class="line">&#123;</span><br><span class="line">&#x2F;* Total length. Must be the first. *&#x2F;</span><br><span class="line">int32 _len;</span><br><span class="line">int32 version;</span><br><span class="line">uint32 nEntry;</span><br><span class="line"></span><br><span class="line">&#x2F;* Varlena array *&#x2F;</span><br><span class="line">MinipageEntry entry[1];</span><br><span class="line">&#125; Minipage;</span><br></pre></td></tr></table></figure><p>默认pg_aoblkdir_XXX的一行可以存储161(NUM_MINIPAGE_ENTRIES)个MinipageEntry信息， 每个MinipageEntry记录一个block的信息，但是为了提高性能，gp也提供如下方法来修改一个entry记录的block数量还有一行记录的entry数量：</p><ol><li>gp_blockdirectory_minipage_size</li></ol><p>一行记录存储的MinipageEntry数量, 取值范围是(1 .. 161)</p><ol start="2"><li>gp_blockdirectory_entry_min_range</li></ol><p>控制一个MinipageEntry 记录的数据范围，取值范围（0, INT_MAX）.<br>计算方法：<br>current_fileOffset - last MinipageEntry.fileOffset &lt; gp_blockdirectory_entry_min_range，则跳过当前block，否则把当前block的相关信息记录到新的MinipageEntry。current_fileOffset - last MinipageEntry.fileOffset 表示的就是last MinipageEntry所能覆盖的范围。<br>gp_blockdirectory_entry_min_range参数会导致MinipageEntry的覆盖范围超过一个block，但是不会小于一个block。<br>pg_aoblkdir_XXX的维护：</p><ol><li>更新<ol><li>insert new tuple时，包含update</li><li>alter table add column时</li><li>build index时，创建新的索引的时候，在scan时会更新pg_aoblkdir_XXX</li></ol></li><li>查询 – index scan， 如何使用pg_aoblkdir_XXX快速定位tuple</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">typedef struct AppendOnlyBlockDirectoryEntry</span><br><span class="line">&#123;</span><br><span class="line">struct range</span><br><span class="line">&#123;</span><br><span class="line">int64fileOffset;     &#x2F;&#x2F; current MinipageEntry.fileOffset</span><br><span class="line">int64firstRowNum;    &#x2F;&#x2F; current MinipageEntry.firstRowNum</span><br><span class="line"></span><br><span class="line">int64afterFileOffset;  &#x2F;&#x2F; next MinipageEntry.fileOffset</span><br><span class="line">int64lastRowNum;       &#x2F;&#x2F; current MinipageEntry.firstRowNum + current MinipageEntry.rowCount - 1</span><br><span class="line">&#125; range;</span><br><span class="line">&#125; AppendOnlyBlockDirectoryEntry;</span><br></pre></td></tr></table></figure><p>根据AOTupleId，算出segfile number和rownum，使用pg_aoblkdir_XXX上的索引，进行索引扫描（条件：segno=segfile number and columngroup_no = column_no and first_row_no &lt;= rownum）快速定位到 AOTupleId 所在的MiniPage。然后通过二分查找在MiniPage中查找rownum所在的block。找到block后，就可以通过fileseek直接读取block，然后遍历block即可找到对应的tuple。</p><h1 id="4-insert-update-delete"><a href="#4-insert-update-delete" class="headerlink" title="4. insert/update/delete"></a>4. insert/update/delete</h1><h2 id="4-1-insert"><a href="#4-1-insert" class="headerlink" title="4.1 insert"></a>4.1 insert</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ExecInsert&#x2F;CopyFrom</span><br><span class="line">aocs_insert_init</span><br><span class="line">  aocs_insert_values</span><br><span class="line">  </span><br><span class="line">ExecEndPlan&#x2F;CopyFrom</span><br><span class="line">aocs_insert_finish</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> <strong>aocs_insert_init</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">初始化AOCSInsertDesc</span><br><span class="line">open DatumStreams</span><br><span class="line">从gp_fastsequence获取rownum range</span><br><span class="line">初始化fist rownum</span><br><span class="line">初始化AppendOnlyBlockDirectory</span><br></pre></td></tr></table></figure></li><li><input checked disabled type="checkbox"> <strong>aocs_insert_values</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for each columns</span><br><span class="line">do</span><br><span class="line">尝试把value存入当前block</span><br><span class="line">  if failed</span><br><span class="line">  把当前block写入buffer，并清空当前block</span><br><span class="line">    更新blockDirectory系统表</span><br><span class="line">  再次尝试把value写入当前block</span><br><span class="line">    if failed</span><br><span class="line"> 把value分割，写入不同的block(lob，large object)</span><br><span class="line">  更新blockDirectory系统表</span><br><span class="line">done</span><br><span class="line">set AOTupleId</span><br><span class="line">如 rownum 使用完了，从新从gp_fastsequence获取一批rownum(100)</span><br></pre></td></tr></table></figure>有几点需要注意的是：</li></ul><ol><li>如果压缩类型是rle_type，则使用lob存储时，不会进行压缩</li></ol><ul><li><input checked disabled type="checkbox"> <strong>aocs_insert_finish</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">for each columns</span><br><span class="line">do</span><br><span class="line">write block to buffer</span><br><span class="line">  update block directory catalog</span><br><span class="line">  close file</span><br><span class="line">  flush buffer content to disk</span><br><span class="line">    sync content to mirror</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">update aocs file segment information</span><br><span class="line">some cleanup job</span><br></pre></td></tr></table></figure><h2 id="4-2-delete"><a href="#4-2-delete" class="headerlink" title="4.2 delete"></a>4.2 delete</h2></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ExecDelete</span><br><span class="line">aocs_delete_init</span><br><span class="line">  aocs_delete</span><br><span class="line">  </span><br><span class="line"> ExecEndPlan</span><br><span class="line"> aocs_delete_finish</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> <strong>aocs_delete_init</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">初始化AOCSDeleteDesc</span><br><span class="line">初始化visibilityMap</span><br><span class="line">初始化visibilityMapDelete</span><br></pre></td></tr></table></figure><ul><li><p><strong>aocs_delete</strong><br>比较简单就是把visibilityMap上对应的位置标记为1<br>主要工作在AppendOnlyVisimapDelete_Hide里面完成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if tuple不在current_Visimap_Entry内</span><br><span class="line">if current_Visimap_Entry is dirty</span><br><span class="line">  把current_Visimap_Entry刷入外存(使用work file组织)</span><br><span class="line">  查找tuple对应的visimap entry并加装到current_Visimap_Entry</span><br><span class="line">标记current_Visimap_Entry 对应的bit为1，并设置为dirty</span><br></pre></td></tr></table></figure><p>对于pg_aovisimap_XXX 系统表的修改，gp这里做了一个特殊处理，原有系统表的修改是通过shared_buffer来做的，没改一次要修改一次shared_buffer并记录xlog。这里gp对于pg_aovisimap_XXX 的修改做了优化，把同一条语句内多次修改pg_aovisimap_XXX 的操作合并成了，这样可以减少pg_aovisimap_XXX 的更新次数已经xlog量。合并的方式是：对应pg_aovisimap_XXX 的修改先记录到内存，如果内存放不下，则写入到work file内，在最后语句结束的是在把内存和work file内的修改更新到pg_aovisimap_XXX 上。<br>这样做的出发点：</p><ol><li>减少pg_aovisimap_XXX由于更新导致的过度膨胀</li><li>减少xlog，可以提高mirror同步效率</li></ol></li><li><p><strong>aocs_delete_finish</strong></p></li></ul><p>主要是把内存中的visimap entry 和work file内的entry 合并然后更新到pg_aovisimap_XXX</p><h2 id="4-3-update"><a href="#4-3-update" class="headerlink" title="4.3 update"></a>4.3 update</h2><p>update操作实际就是 delete + insert，代码上也基本是这样实现的，没啥好说的。<br>有一点是，虽然是列存，但是update的时候还是按行来做的，即使我们只update某一列，实际上与行存一样，aocs也是把整个行标记为delete，然后insert新的行。</p><h2 id="4-4-copy-from"><a href="#4-4-copy-from" class="headerlink" title="4.4 copy from"></a>4.4 copy from</h2><p>与insert 类似</p><h2 id="4-5-iud并发控制"><a href="#4-5-iud并发控制" class="headerlink" title="4.5 iud并发控制"></a>4.5 iud并发控制</h2><ul><li><input checked disabled type="checkbox"> <strong>update/delete并发控制</strong></li></ul><p>update/delete并发控制比较简单，在执行时加表的ExclusiveLock, 把update/delete/ select for update操作串行化。<br>这样做的目的主要是防止死锁，由于gp没有全局的死锁检测机制，所以弱化update并发来解决死锁问题。<br>目前gp死锁检测处理不了的情况：<br><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/global-deadlock.png" alt="global-deadlock"></p><ul><li><input checked disabled type="checkbox"> <strong>insert与insert的并发控制</strong></li></ul><p>对于同一个表的insert是可以并发执行的，由于列存与pg heap表不同，gp针对列存表单独实现了一个比较简单粗暴的并发控制机制：对于insert操作，每个insert进程处理不同的segfile，即insert 进程直接不会有冲突，所以也不需要加锁等控制。简单来说有2点：</p><ol><li>对于列存文件的修改，每个insert进程修改不同的segfile</li></ol><p>在insert之前，gp会为insert进程分配当前进程插入的segfile，不同的insert 进程插入不同的segfile，处理流程可以看一下SetSegnoForWrite函数的实现。</p><ol start="2"><li>对应meta表的修改，走pg heap 表，使用pg原有的并发控制机制</li></ol><ul><li><input checked disabled type="checkbox"> <strong>insert与update/delete的并发控制</strong></li></ul><p>由于update/delete需要加ExclusiveLock锁，导致insert与update/delete也无法并行执行</p><h1 id="5-scan"><a href="#5-scan" class="headerlink" title="5. scan"></a>5. scan</h1><h2 id="5-1-table-scan"><a href="#5-1-table-scan" class="headerlink" title="5.1 table scan"></a>5.1 table scan</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">aocs_beginscan</span><br><span class="line">while !done</span><br><span class="line">aocs_getnext</span><br><span class="line">aocs_endscan</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> <strong>aocs_beginscan</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">resourceowner table refcount+1</span><br><span class="line">GetAppendOnlyEntry</span><br><span class="line">从pg_aoseg_xxx表查询所有的segfile信息</span><br><span class="line">create AOCSScanDesc</span><br><span class="line">aocs_initscan</span><br><span class="line">  open_ds_read</span><br><span class="line">  AppendOnlyVisimap_Init</span><br></pre></td></tr></table></figure>关于scan时的snapshot：</li></ul><ol><li>current_snapshot， 正常查询的snapshot，scan之前生成</li></ol><p>tabledata_snapshot: current_snapshot<br>metadata_snapshot: current_snapshot</p><ol start="2"><li>SnapshotAny，开启gp_select_invisible后使用，用于查询历史数据。</li></ol><p>tabledata_snapshot: SnapshotAny<br>aocs表tuple上是没有事务信息的，这个snapshot主要是用来配合pg_visimap_xxx系统表的<br>metadata_snapshot: current_snapshot</p><ul><li><p><strong>aocs_endscan</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">resourceowner table refcount-1</span><br><span class="line">destory AOCSScanDesc</span><br><span class="line">close_cur_scan_seg</span><br><span class="line">  close_ds_read</span><br><span class="line">  AppendOnlyVisimap_Finish</span><br></pre></td></tr></table></figure></li><li><p><strong>aocs_getnext</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">READ_NEXT:</span><br><span class="line"><span class="keyword">if</span> necessary, open next segfile;</span><br><span class="line">    <span class="keyword">if</span> (No more seg)</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">  foreach_columns</span><br><span class="line">  &#123;</span><br><span class="line">  <span class="keyword">if</span> (scan-&gt;proj[i] == <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">// 查询没有使用到该column，skip read</span></span><br><span class="line">    <span class="keyword">continue</span>;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">//当前block的current_tuple 后移，类似于jdbc ResultSet.next</span></span><br><span class="line">    datumstreamread_advance;</span><br><span class="line">    <span class="keyword">if</span> (no tuple)</span><br><span class="line">    &#123;</span><br><span class="line">            <span class="comment">// read next block</span></span><br><span class="line">            datumstreamread_block;</span><br><span class="line">            <span class="keyword">if</span> (end of file)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">goto</span> READ_NEXT;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> necessary, insert block directory meta entry;</span><br><span class="line">            datumstreamread_advance;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// get current column value</span></span><br><span class="line">        datumstreamread_get</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">set</span> AOTupleId</span><br><span class="line">    数据可见性判断，如果不可见 <span class="keyword">goto</span> READ_NEXT;</span><br><span class="line">    save current tuple <span class="keyword">and</span> AOTupleId;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ol><li>对于查询未涉及的column，read时自动跳过，减少io</li><li>数据可见性判断<ul><li>如果tabledata_snapshot是SnapshotAny，则跳过可见性判断</li><li>否则，通过visimap判断tuple是否已经被删除</li></ul></li></ol><ul><li><input checked disabled type="checkbox"> <strong>aocs_getnext_comp</strong></li></ul><p>开启向量化后，aocs scan接口。与aocs_getnext的不同之处在于：</p><ol><li>aocs_getnext每次读取一个tuple，然后就返回，而aocs_getnext_comp是每次读取一批tuple。</li><li>对于向量化，会有一些谓词下推，所以aocs_getnext_comp会做一些filter<h2 id="5-2-index-scan"><a href="#5-2-index-scan" class="headerlink" title="5.2 index scan"></a>5.2 index scan</h2>aocs上的索引扫描<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BitmapAppendOnlyScanNext</span><br><span class="line">aocs_fetch_init</span><br><span class="line">  aocs_fetch</span><br><span class="line">aocs_fetch_finish</span><br></pre></td></tr></table></figure>aocs_fetch：<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">foreach_columns</span><br><span class="line">&#123;</span><br><span class="line">从BlockDirectoryEntry找到tuple所在block</span><br><span class="line">    从block中读取tuple</span><br><span class="line">    判断tuple可见性</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>可优化的地方：</p><ol><li>先做可见性判断在读block</li></ol><p>目前做可见性判断时是先把tuple所在block 从disk读到内存，然后在判断可见性。实际上aocs的可见性信息是记录在pg_aovisimap表里面的，可以先判断可见性，再读block，这样如果不可见可以减少不必须的io。</p><ol start="2"><li>第一列通过可见性判断后，后面列可以跳过可见性判断</li></ol><p>rownum是tuple的唯一标识，如果tuple的第一列通过了可见性判断，则后面所有的列也是可见的，因此是不需要再做可见性判断的</p><h2 id="5-3-header-scan"><a href="#5-3-header-scan" class="headerlink" title="5.3 header scan"></a>5.3 header scan</h2><p>alter table add column 是调用，粗略看了一下代码，主要是用来辅助创建新的column对应的segfile的。header scan实际上扫描的时候只是把block读取出来，并不会把block解压，主要是使用block上的header信息。<br>有两点：</p><ol><li>ao表 添加新的column时，必须指定default value</li><li>新segfile与之前column的segfile在结构完基本一样，比如每个block记录多少个tuple等<h2 id="5-4-tid-scan"><a href="#5-4-tid-scan" class="headerlink" title="5.4 tid scan"></a>5.4 tid scan</h2>ao表没有tid scan，只有heap表有<h2 id="5-5-range-scan"><a href="#5-5-range-scan" class="headerlink" title="5.5 range scan"></a>5.5 range scan</h2>主要是做compaction 时使用。与 table scan类似，不同之处是每次只扫描指定的几个segfile，而不是扫描所有的segfile。<h1 id="6-事务ACID"><a href="#6-事务ACID" class="headerlink" title="6. 事务ACID"></a>6. 事务ACID</h1>aocs实现了类似heap的mvcc，并且支持完整的事务ACID。aocs表数据分为两种：一是用户数据，二是meta数据。用户数据存储在segfile中，只存储数据，不存储任何事务信息(与heap表不同)，commit之前一定会flush到磁盘；meta数据存储在heap表中，通过分布式事务保证。<br>aocs的实现事务的meta表有2个 pg_aocsseg_xxx和pg_aovisimap_xxx，其中pg_aocsseg_xxx记录数据文件的逻辑长度(即当前事务可以看到的长度)；pg_aovisimap_xxx记录delete tuple，即哪些tuple被删除了。</li></ol><ul><li><input checked disabled type="checkbox"> <strong>insert</strong></li></ul><p><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/insert.png" alt="insert"></p><ul><li><input checked disabled type="checkbox"> <strong>update</strong></li></ul><p><img src="/2020/12/28/Greenplum-%E5%88%97%E5%AD%98%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/update.png" alt="update"></p><h1 id="7-aocs-compaction"><a href="#7-aocs-compaction" class="headerlink" title="7. aocs compaction"></a>7. aocs compaction</h1><p>类似于heap表的vacuum，用来清理 delete的tuple或者rollback后残留的tuple。<br>gp对vacuum操作进行了改造，目的是为了适应分布式系统，把原来pg的一个vacuum分成了几个阶段，每个阶段使用一个分布式事务。<br>对于ao表分为4个阶段：</p><ol><li>prepare phase</li></ol><p>truncate unnecessary blocks after the logical EOF</p><ol start="2"><li>compaction phase</li><li>drop phase</li></ol><p>删除compaction phase的segfile</p><ol start="4"><li>cleanup phase</li></ol><p>does normal heap vacuum on auxiliary relations (toast, aoseg, block directory, visimap,) as well as updating stats info in catalog</p><p>compaction主要有两种操作：</p><ol><li>AOCSCompaction_DropSegmentFile</li></ol><p>删掉AOSEG_STATE_AWAITING_DROP状态的segfile。在vacuum的drop phase做。</p><ol start="2"><li>AOCSTruncateToEOF</li></ol><p>在不满足compaction条件时，做truncate，这操作主要是清理最近rollback的事务残留在文件尾部的tuple。</p><ol start="3"><li>AOCSSegmentFileFullCompaction</li></ol><ul><li><input checked disabled type="checkbox"> 触发sql</li></ul><p>vacuum<br>vacuum full</p><ul><li><input checked disabled type="checkbox"> 哪些情况会做</li></ul><p>AppendOnlyCompaction_ShouldCompact</p><ol><li>vacuum full 且有delete</li><li>删除的tuple所占比例大于gp_appendonly_compaction_threshold</li></ol><ul><li><input checked disabled type="checkbox"> 如何做compaction</li></ul><p>compaction的做法比较简单：</p><ol><li>选择一个segfile做compaction，然后再选择一个segfile作为insert segfile。</li><li>遍历compaction segfile的每个tuple，如果可见(snapshot为SnapshotNow)则插入到insert segfile，如果不可见则删除。</li><li>set compaction segfile为AOSEG_STATE_AWAITING_DROP</li><li>delete visimap和block directory 中对应segno的数据</li></ol><p>insert segfile的选择：<br>选择tuple数最小的segfile或者当前vacuum事务正在使用的segfile作为insert segfile。</p><ul><li><input checked disabled type="checkbox"> 相关guc参数<table><thead><tr><th>参数名</th><th></th></tr></thead><tbody><tr><td>gp_appendonly_compaction</td><td>是否开启compaction，如果是off，则vacuum时只需TruncateToEOF</td></tr><tr><td>gp_appendonly_compaction_threshold</td><td>compaction 的阈值</td></tr><tr><td>Debug_appendonly_print_compaction</td><td>输出debug信息</td></tr></tbody></table></li></ul><ol start="8"><li>aocs表的一些限制</li></ol><ul><li><input checked disabled type="checkbox"> <strong>不支持unique index</strong></li></ul><p>不支持的原因是在检查到冲突的tuple后，无法判断该tuple的事务信息。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-相关代码&quot;&gt;&lt;a href=&quot;#1-相关代码&quot; class=&quot;headerlink&quot; title=&quot;1. 相关代码&quot;&gt;&lt;/a&gt;1. 相关代码&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;src&amp;#x2F;backend&amp;#x2F;access&amp;#x2F;aocs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    aocs_compaction.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    aocsam.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    aocssegfiles.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &amp;#x2F;appendonly&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonlyam.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    aomd.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    aosegfiles.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonly_compaction.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonly_visimap.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonly_visimap_entry.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonly_visimap_store.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonly_visimap_udf.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonlyblockdirectory.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonlytid.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    appendonlywriter.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;src&amp;#x2F;backend&amp;#x2F;utils&amp;#x2F;datumstream&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    datumstream.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    datumstreamblock.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;src&amp;#x2F;backend&amp;#x2F;cdb&amp;#x2F;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                cdbbufferedappend.c&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                cdbbufferedread.c&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="aocs" scheme="https://zisedeqing.github.io/categories/greenplum6-0/aocs/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="aocs" scheme="https://zisedeqing.github.io/tags/aocs/"/>
    
    <category term="列存" scheme="https://zisedeqing.github.io/tags/%E5%88%97%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>Greenplum 6.0 HA</title>
    <link href="https://zisedeqing.github.io/2020/12/28/Greenplum-6-0-HA/"/>
    <id>https://zisedeqing.github.io/2020/12/28/Greenplum-6-0-HA/</id>
    <published>2020-12-28T12:44:34.000Z</published>
    <updated>2020-12-29T01:32:53.318Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-整体架构"><a href="#1-整体架构" class="headerlink" title="1. 整体架构"></a>1. 整体架构</h1><p>Greenplum 6.0 在HA的架构上与4.3基本上一样的，改动比较大的是primary/mirror之间的数据同步机制，由4.3的file replication 改为streaming replication。</p><a id="more"></a><p>整体架构如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/arch.png" alt="整体架构图"><br>Greenplum 集群有一个master和若干个segment组成，master有一个standby做备份，实时同步master上的元数据变更，每个segment都有一个mirror节点，通过流复制实时同步segment上的元信息和数据变更。master/standby和segment的primary/mirror的流复制结构图如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/arch2.png" alt="primary-mirror"><br>在master或者primary节点上，用户backend产生XLOG并修改数据，wal sender进程 读取用户新产生的xlog数据，然后通过流复制协议发送到standby或者mirror节点的wal receiver进程。wal receiver进程接收wal sender发送过来的xlog record，并把xlog record写入xlog 文件，startup进程读取wal sender写入的xlog文件，实时redo产生的xlog record，并把数据变更写入文件。</p><h1 id="2-streaming-replication"><a href="#2-streaming-replication" class="headerlink" title="2. streaming replication"></a>2. streaming replication</h1><p>streaming replication实际上是由2部分组成，一是base backup和流复制协议，base backup是流复制的基本，在primary/master的snapshot上，把增量修改通过流复制协议同步给standby/mirror。</p><h2 id="2-1-base-backup"><a href="#2-1-base-backup" class="headerlink" title="2.1 base backup"></a>2.1 base backup</h2><p>base backup 主要是对primary/segment通过在线的方式对于节点做一个snapshot，以供后续根据该snapshot搭建standby或者PITR(Point-in-Time Recovery)。</p><p>pg提供的机制做base backup的方式如下：</p><ol><li>pg_basebackup 命令行工具</li></ol><p>实际上内部还是通过流复制协议的BASE_BACKUP命令实现的</p><ol start="2"><li>系统内置函数：pg_start_backup，pg_stop_backup</li></ol><p>这种方式目前gp6不支持。</p><ol start="3"><li>通过流复制协议发送BASE_BACKUP命令</li></ol><p>对于内部测试来说，可以通过如下命令执行base backup</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PGOPTIONS=<span class="string">&quot;-c gp_session_role=utility&quot;</span> psql <span class="string">&quot;dbname=postgres replication=database&quot;</span> -c <span class="string">&quot;BASE_BACKUP LABEL &#x27;test_bk&#x27;&quot;</span></span><br></pre></td></tr></table></figure><p>base backup的代码实现主要在perform_base_backup函数内，主要做如下的工作：</p><ol><li>do_pg_start_backup</li></ol><p><img src="/2020/12/28/Greenplum-6-0-HA/do_pg_start_backup.png" alt="do_pg_start_backup"><br>do_pg_start_backup 主要的目的就是要做一次checkpoint，以便后续standby的的redo。</p><ul><li>full page write</li></ul><p>强制做full page write的目的是防止在后面copy 数据的时候读取到未刷完整的page</p><ul><li>switch xlog</li></ul><p>确保checkpoint写入的xlog的timeline id是当前的timeline id。详细的原因可以查看do_pg_start_backup的注释</p><ul><li>create backup label file</li></ul><p>backup label file 不一定会创建，如果是通过流复制协议的BASE_BACKUP命令，就不会创建该文件，但是会把文件的内容发送到client端。backup label file 内容如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/backup-label-file.png" alt="backup label file"></p><ul><li>CHECKPOINT LOCATION</li></ul><p>do_pg_start_backup触发的checkpoint lsn，用于PITR，流复制没有用到该字段</p><ul><li>START WAL LOCATION</li></ul><p>流复制日志的起点位置，在流复制时，wal sender会把该位置之后的xlog 发送给wal receiver。<br>该位置时间上是checkpoint的redo位置，所以也可以理解为，该位置之前的xlog记录的数据库变更都已经flush到磁盘了，因此也就不需要了。</p><ul><li>BACKUP METHOD</li></ul><p>使用何种防止做的base backup：pg_start_backup | streamed<br>对于Greenplum而言，目前应该只有streamed。</p><ul><li>BACKUP FROM</li></ul><p>该backup是对primary 还是 standby做的，这里的primary是postgresql的primary，不是Greenplum的primary segment。<br>对于Greenplum而言，目前应该只有primary。</p><ul><li>START TIME</li></ul><p>backup的时间</p><ul><li>LABEL </li></ul><p>当前backup的label</p><ol start="2"><li>WalSndSetXLogCleanUpTo</li></ol><p>把start wal location更新到wal sender状态里面，该操作的主要目的是防止在wal sender 同步xlog之前，checkpoint回收xlog时，把start wal location之后的xlog给回收掉。<br>对于pg9.4是没有这块代码的，该代码是greenplum 添加的，我找了一下提交记录，在Greenplum 4.3的时候就已经存在了，看来是代码合并的时候直接就把这块代码留下来了。<br>当时Greenplum 4.3添加这块代码的原因是Greenplum 4.3的内核版比较低，master通过流复制与standby同步时，有可能出现还未通过到standby的xlog在master上备回收了。但是在Greenplum 6.0中，可以使用replication slot来解决这个问题，不理解这一块的代码为什么还要保留。</p><ol start="3"><li>send data to client</li></ol><p>把primary/master的数据发送到client端</p><ol start="4"><li>do_pg_stop_backup</li></ol><p><img src="/2020/12/28/Greenplum-6-0-HA/do_pg_stop_backup.png" alt="do_pg_stop_backup"><br>do_pg_stop_backup主要做的事情是把do_pg_start_backup修改的状态重置，并且插入一个backup end xlog，该xlog在standby或者PITR时，用来检查数据一致性</p><ol start="5"><li>if need, send wal file to client</li></ol><h2 id="2-2-streaming-replication"><a href="#2-2-streaming-replication" class="headerlink" title="2.2 streaming replication"></a>2.2 streaming replication</h2><p><img src="/2020/12/28/Greenplum-6-0-HA/streaming-replication.png" alt="streaming replication"><br>上图展示了master/primary与standby/mirror如何通过streaming replication来实现数据同步的：</p><ol><li>master/primary的user backend 执行用户sql，修改数据并产生XLOG，在提交之前backend进程通知wal sender进程把新产生的xlog 同步到 standby/mirror。</li><li>wal sender 进程循环读取xlog file，如果有新的xlog产生，则把xlog 通过流复制协议发送给standby/mirror。</li></ol><p>在发送之前，会先处理standby/mirror发回的replay消息，用于跟踪standby/mirror的xlog write or flush状况。</p><ol start="3"><li>standby/mirror的wal receiver进程收到wal sender进程发送的xlog record后，把xlog record 写入xlog file，并flush到磁盘，然后通知recovery进程，有新的xlog。</li><li>recovery进程循环读取xlog file，如果有新的xlog，则redo。</li></ol><p>streaming replication是master/pirmary产生日志并且flush到xlog 文件之后wal sender才能把xlog 同步到standby/mirror。所以这里还是有一个时间差的，即使在强同步的模式下，如果master/primary在flush xlog后宕掉了，这些flush的xlog是有可能没有同步到standby/mirror上的。</p><h3 id="2-2-1-SyncRepWaitForLSN"><a href="#2-2-1-SyncRepWaitForLSN" class="headerlink" title="2.2.1 SyncRepWaitForLSN"></a>2.2.1 SyncRepWaitForLSN</h3><p>SyncRepWaitForLSN 是用户backend 在事务prepare/commit/abort阶段等待当前事务的xlog全部同步到standby/mirror的接口，该函数调用栈：<br><img src="/2020/12/28/Greenplum-6-0-HA/SyncRepWaitForLSN.png" alt="SyncRepWaitForLSN"><br>SyncRepWaitForLSN函数的主要功能是根据用户配置的同步模式，等待xlog 同步到standby/mirror。</p><ul><li><input checked disabled type="checkbox"> <strong>synchronous_commit</strong></li></ul><p>通过guc参数配置同步级别，级别从低到高：</p><table><thead><tr><th>SYNCHRONOUS_COMMIT_OFF</th><th>异步提交</th></tr></thead><tbody><tr><td>SYNCHRONOUS_COMMIT_LOCAL_FLUSH</td><td>等待本地xlog flush，即primary/master flush xlog</td></tr><tr><td>SYNCHRONOUS_COMMIT_REMOTE_WRITE</td><td>等待standby/mirror write xlog</td></tr><tr><td>SYNCHRONOUS_COMMIT_REMOTE_FLUSH</td><td>等待standby/mirror flush xlog</td></tr></tbody></table><p>默认是SYNCHRONOUS_COMMIT_REMOTE_FLUSH。</p><ul><li><input checked disabled type="checkbox"> <strong>wait 时的中断处理</strong></li></ul><p>有两种中断：</p><ul><li>canceling query    </li></ul><p>对于这个中断，Greenplum 和pg的处理不太一样，pg是抛出WARNING，并取消等待；而gp是直接忽略该中断，继续等待。<br>从代码的注释看，Greenplum这样做的原因是既然用户配置的同步模式，就一定要保证primary 与mirror的同步，而且Greenplum有fts服务，可以自动检测到mirror是否挂掉了，这样gp可以解开因为等待xlog 同步的事务。</p><ul><li>proc die</li></ul><p>如果是master，则抛出WARNING，并取消等待<br>如果是primary，则FATAL当前backend。</p><h3 id="2-2-2-wal-sender"><a href="#2-2-2-wal-sender" class="headerlink" title="2.2.2 wal sender"></a>2.2.2 wal sender</h3><p>wal sender是master/primary上的xlog 发送进程，用于把新产生的xlog 发送到wal receiver端。wal sender 进程的创建是由standby/mirror链接master/primary而创建的，处于测试的目的，可以通如下的命令创建wal sender：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PGOPTIONS=<span class="string">&quot;-c gp_session_role=utility&quot;</span> psql <span class="string">&quot;dbname=postgres replication=database&quot;</span></span><br></pre></td></tr></table></figure><h4 id="2-2-2-1-replication-command"><a href="#2-2-2-1-replication-command" class="headerlink" title="2.2.2.1 replication command"></a>2.2.2.1 replication command</h4><p>wal sender支持多个流复制命令来实现各种功能, 通过上面的psql命令行可以执行这些command：</p><ol><li>identify_system</li></ol><p>返回系统标志，由一下字段组成：<br>systemid | timeline | xlogpos | dbname</p><ol start="2"><li>base_backup</li></ol><p>执行base backup，在上面的章节已经介绍过了。</p><ol start="3"><li>start_replication</li></ol><p>启动物理流复制，即wal sender进程，循环发送xlog。</p><ol start="4"><li>start_logical_replication</li></ol><p>启动逻辑复制流复制，与物理流复制不同的是发送逻辑日志，而不是xlog</p><ol start="5"><li>create_replication_slot</li></ol><p>创建一个replication slot，replication slot会在后面章节介绍</p><ol start="6"><li>drop_replication_slot</li></ol><p>删除replication slot</p><ol start="7"><li>timeline_history</li></ol><p>发送time line history 文件到client端</p><h4 id="2-2-2-2-start-replication"><a href="#2-2-2-2-start-replication" class="headerlink" title="2.2.2.2 start replication"></a>2.2.2.2 start replication</h4><p>start replication 用于启动流复制，command语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">START_REPLICATION [SLOT slot_name] [PHYSICAL] XXX&#x2F;XXX [TIMELINE tli]</span><br></pre></td></tr></table></figure><ul><li>timeline 表示要复制的xlog的时间线，如果是历史的时间线，则把改时间线内的xlog复制完后，就结束</li><li>startpos xlog复制的起点，一般是base bakcup中返回的start wal location</li><li><input checked disabled type="checkbox"> <strong>wal sender state</strong></li></ul><p>wal sender 通过状态机来工作，状态转换如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/wal-sender-state.png" alt="wal sender state"><br>WALSNDSTATE_STARTUP： 表示wal sender 已经启动，等待输入<br>WALSNDSTATE_BACKUP： 表中wal sender正在做base backup<br>WALSNDSTATE_CATCHUP：表示wal sender正在发送xlog，standby/mirror正在追赶master/primary的xlog<br>WALSNDSTATE_STREAMING：表示standby/mirror与master/primary的xlog处于同步状态。<br>处于该状态的wal sender才可能会成为sync的。当第一次把所有的xlog发送给wal receiver后，从WALSNDSTATE_CATCHUP转换成WALSNDSTATE_STREAMING。</p><p>WALSNDSTATE_STOPPING: wal sender 正在关闭</p><p>可以通过pg_stat_get_wal_senders函数查看wal sender的状态信息。</p><ul><li><input checked disabled type="checkbox"> <strong>wal send main loop</strong></li></ul><p>wal send main loop 流程图如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/wal-send-main-loop.png" alt="wal send main loop"><br>wal sender不断的读取xlog file并发送的wal receiver端，如果没有xlog可读或者上一次的数据还未发送完，则wal sender会处于block状态，唤醒的事件有：WL_LATCH_SET | WL_POSTMASTER_DEATH | WL_TIMEOUT |    WL_SOCKET_READABLE。</p><ul><li><input checked disabled type="checkbox"> <strong>standby/mirror reply</strong></li></ul><p>‘c’ 报文： standby/mirror 结束流复制<br>‘X’ 报文:  standby/mirror socket 关闭<br>‘d’ 报文：状态报文，会把standby/mirror的状态，有2个功能：<br>‘r’:  汇报xlog flush和redo信息，有如下项：<br>xlog writer location<br>xlog flush location<br>xlog redo location<br>wal sender 收到这些信息后，可以根据这些信息唤醒正在等待提交的事务，或者防止xlog被过早的回收（Greenplum不会出现这个问题）<br>‘h’:  hot standby feedback，Greenplum不支持hot standby<br>防止master过早的回收standby可能还需要查询的tuple 版本。<br>这个问题是这样的，vacuum在清理tuple无效版本时，一般是把所有正在运行的事务都不可见的tuple版本清理掉，但是master在做时，只能确定master上所有的事务不可见，但是无法确定tuple版本在standby上是否有事务可见。如果master不理standby而直接把standby上有事务可见的tuple版本清理到，则wal sender把这些更改同步到standby上后，standby的query可能会报错：ERROR: canceling statement due to conflict with recovery。<br>postgresql的解决方案是：hot standby定期汇报自己的oldest xid，master 在清理tuple时，把standby上的事务也考虑在内。</p><h4 id="2-2-2-3-相关系统表"><a href="#2-2-2-3-相关系统表" class="headerlink" title="2.2.2.3 相关系统表"></a>2.2.2.3 相关系统表</h4><ol><li>pg_stat_replication</li></ol><p>postgresql内置系统视图，可以查询单节点上的wal sender状态，如下图所示：<br><img src="/2020/12/28/Greenplum-6-0-HA/pg_stat_replication.png" alt="pg_stat_replication"></p><ol start="2"><li>gp_stat_replication<br>greemplum6.0 内置系统视图，实际上是对pg_stat_replication的封装：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> gp_stat_get_master_replication() <span class="keyword">RETURNS</span> SETOF RECORD <span class="keyword">AS</span></span><br><span class="line">$$</span><br><span class="line">    <span class="keyword">SELECT</span> pg_catalog.gp_execution_segment() <span class="keyword">AS</span> gp_segment_id, <span class="operator">*</span></span><br><span class="line">    <span class="keyword">FROM</span> pg_catalog.pg_stat_replication</span><br><span class="line">$$</span><br><span class="line"><span class="keyword">LANGUAGE</span> <span class="keyword">SQL</span> <span class="keyword">EXECUTE</span> <span class="keyword">ON</span> MASTER;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> gp_stat_get_segment_replication() <span class="keyword">RETURNS</span> SETOF RECORD <span class="keyword">AS</span></span><br><span class="line">$$</span><br><span class="line">    <span class="keyword">SELECT</span> pg_catalog.gp_execution_segment() <span class="keyword">AS</span> gp_segment_id, <span class="operator">*</span></span><br><span class="line">    <span class="keyword">FROM</span> pg_catalog.pg_stat_replication</span><br><span class="line">$$</span><br><span class="line"><span class="keyword">LANGUAGE</span> <span class="keyword">SQL</span> <span class="keyword">EXECUTE</span> <span class="keyword">ON</span> <span class="keyword">ALL</span> SEGMENTS;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> gp_stat_get_segment_replication_error() <span class="keyword">RETURNS</span> SETOF RECORD <span class="keyword">AS</span></span><br><span class="line">$$</span><br><span class="line">    <span class="keyword">SELECT</span> pg_catalog.gp_execution_segment() <span class="keyword">AS</span> gp_segment_id, pg_catalog.gp_replication_error() <span class="keyword">as</span> sync_error</span><br><span class="line">$$</span><br><span class="line"><span class="keyword">LANGUAGE</span> <span class="keyword">SQL</span> <span class="keyword">EXECUTE</span> <span class="keyword">ON</span> <span class="keyword">ALL</span> SEGMENTS;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> gp_stat_replication <span class="keyword">AS</span></span><br><span class="line">    <span class="keyword">SELECT</span> <span class="operator">*</span>, pg_catalog.gp_replication_error() <span class="keyword">AS</span> sync_error</span><br><span class="line">    <span class="keyword">FROM</span> pg_catalog.gp_stat_get_master_replication() <span class="keyword">AS</span> R</span><br><span class="line">    (gp_segment_id <span class="type">integer</span>, pid <span class="type">integer</span>, usesysid oid,</span><br><span class="line">     usename name, application_name text, client_addr inet, client_hostname text,</span><br><span class="line">     client_port <span class="type">integer</span>, backend_start timestamptz, backend_xmin xid, state text,</span><br><span class="line">     sent_location pg_lsn, write_location pg_lsn, flush_location pg_lsn,</span><br><span class="line">     replay_location pg_lsn, sync_priority <span class="type">integer</span>, sync_state text)</span><br><span class="line">    <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">SELECT</span> G.gp_segment_id</span><br><span class="line">            , R.pid, R.usesysid, R.usename, R.application_name, R.client_addr</span><br><span class="line">            , R.client_hostname, R.client_port, R.backend_start, R.backend_xmin, R.state</span><br><span class="line">    , R.sent_location, R.write_location, R.flush_location</span><br><span class="line">    , R.replay_location, R.sync_priority, R.sync_state, G.sync_error</span><br><span class="line">        <span class="keyword">FROM</span> (</span><br><span class="line">            <span class="keyword">SELECT</span> E.<span class="operator">*</span></span><br><span class="line">            <span class="keyword">FROM</span> pg_catalog.gp_segment_configuration C</span><br><span class="line">            <span class="keyword">JOIN</span> pg_catalog.gp_stat_get_segment_replication_error()</span><br><span class="line">    <span class="keyword">AS</span> E (gp_segment_id <span class="type">integer</span>, sync_error text)</span><br><span class="line">            <span class="keyword">ON</span> c.content <span class="operator">=</span> E.gp_segment_id</span><br><span class="line">            <span class="keyword">WHERE</span> C.role <span class="operator">=</span> <span class="string">&#x27;m&#x27;</span></span><br><span class="line">        ) G</span><br><span class="line">        <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> pg_catalog.gp_stat_get_segment_replication() <span class="keyword">AS</span> R</span><br><span class="line">        (gp_segment_id <span class="type">integer</span>, pid <span class="type">integer</span>, usesysid oid,</span><br><span class="line">         usename name, application_name text, client_addr inet,</span><br><span class="line"> client_hostname text, client_port <span class="type">integer</span>, backend_start timestamptz,</span><br><span class="line"> backend_xmin xid, state text, sent_location pg_lsn,</span><br><span class="line"> write_location pg_lsn, flush_location pg_lsn,</span><br><span class="line">         replay_location pg_lsn, sync_priority <span class="type">integer</span>, sync_state text)</span><br><span class="line">         <span class="keyword">ON</span> G.gp_segment_id <span class="operator">=</span> R.gp_segment_id</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><p>该视图可以在master上执行，收集master和所有segment的replication信息，如下图所示：<br><img src="/2020/12/28/Greenplum-6-0-HA/gp_stat_replication.png" alt="gp_stat_replication"></p><ol start="3"><li>pg_xlog_location_diff</li></ol><p>可以通过如下sql查看mirror/standby 的日志落后了primary/master多少：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">gp_execution_segment() <span class="keyword">as</span> gp_segment_id, </span><br><span class="line">  pg_xlog_location_diff(pg_current_xlog_location(),replay_location)<span class="operator">/</span><span class="number">1024</span><span class="operator">/</span><span class="number">1024</span> <span class="keyword">as</span> MB</span><br><span class="line"><span class="keyword">from</span> gp_dist_random(<span class="string">&#x27;pg_stat_replication&#x27;</span>);</span><br></pre></td></tr></table></figure><h3 id="2-3-2-wal-receiver"><a href="#2-3-2-wal-receiver" class="headerlink" title="2.3.2 wal receiver"></a>2.3.2 wal receiver</h3><p>wal receiver 是standby/mirror的后台进程，在recovery进程redo完现有日志后，会通过RequestXLogStreaming 接口启动wal receiver进程，链接master/primary 启动wal sender进程同步xlog。<br>wal receiver流程图如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/wal-receiver.png" alt="wal receiver"><br>wal receiver 有2层循环，内层循环处理某一个时间线内的xlog的receive，当该时间线内的xlog处理完后，wal sender会停止发送xlog，wal receiver会从master/primary上拉取 最新的timeline history，然后等待startup 进程确定下一个时间线的xlog start location。<br>外层循环处理所有时间线的xlog，每次都会依次发送IDENTIFY_SYSTEM、START_REPLICATION command，从新开始新一个时间线的复制。</p><p>为什么在receive xlog时，会有多个时间线呢？</p><ol><li>cascade replication</li></ol><p>对于级联的standby，如果standby promote为master后，时间线为增加，该standby的standby就需要处理不同时间线的xlog</p><ol start="2"><li>基于比较早的备份做recovery</li></ol><p>如果base backup做的比较早，但是recovery或者以standby启动的时候比较晚，master/standby做过切换后，也会出现这个现象。这个需要master有xlog archive才行，如果没有xlog archive，之前的base backup有可能会找不到需要的xlog。</p><ol start="3"><li>因为bug导致的</li></ol><p>在WaitForWALToBecomeAvailable代码的注释中提到：<br><img src="/2020/12/28/Greenplum-6-0-HA/WaitForWALToBecomeAvailable.png" alt="WaitForWALToBecomeAvailable"></p><h3 id="2-4-2-startup-process"><a href="#2-4-2-startup-process" class="headerlink" title="2.4.2 startup process"></a>2.4.2 startup process</h3><p>startup 进程是standby/mirror上用于xlog replay进程，在standby/mirror启动时，startup进程启动，根据recovery.conf中的配置，确定recovery方式，然后read xlog并redo xlog，如果pg_xlog目录下的xlog都已经redo完，并且standbymode 为on的话，调用RequestXLogStreaming接口启动wal receiver，从master/primary获取xlog，wal receiver收到xlog后，在通知startup 进行redo，如此循环，直到standby promote为master/primary。<br>startup process 主函数为StartupXLOG，该函数无论是用于启动时做xlog replay的主函数。对于postgresql来说只有xlog redo到一致状态后，系统才可以接收服务。对于primary/master，StartupXLOG只是redo pg_xlog目录下面的xlog，对于standby或者其他recovery方式(PITR) 来说值redo pg_xlog 下的日志是不够的。postgresql实现了一个状态机，用来控制xlog的来源，该代码实现在WaitForWALToBecomeAvailable函数内。<br><img src="/2020/12/28/Greenplum-6-0-HA/WaitForWALToBecomeAvailable2.png" alt="WaitForWALToBecomeAvailable state"><br>XLOG_FROM_ANY： 需要的xlog可以是任何来源：archive, pg_xlog,stream,甚至是用户手动copy到pg_xlog下，该状态不参与状态的转换。<br>XLOG_FROM_ARCHIVE：从restore_command从获取xlog<br>XLOG_FROM_PG_XLOG：需要的xlog在pg_xlog目录下<br>XLOG_FROM_STREAM：需要从master/primary 获取xlog</p><h3 id="2-5-2-streaming-replication-protocal"><a href="#2-5-2-streaming-replication-protocal" class="headerlink" title="2.5.2 streaming replication protocal"></a>2.5.2 streaming replication protocal</h3><p>流复制协议，用于实现流复制功能，协议的具体细节可以参考postgresql的官方文档：<a href="https://www.postgresql.org/docs/9.4/protocol-replication.html">https://www.postgresql.org/docs/9.4/protocol-replication.html</a>。<br>如果想要测试流复制写，可以使用psql执行流复制的command，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PGOPTIONS=<span class="string">&quot;-c gp_session_role=utility&quot;</span> psql <span class="string">&quot;dbname=postgres replication=database&quot;</span> -h host -p port</span><br></pre></td></tr></table></figure><h1 id="3-replication-slot"><a href="#3-replication-slot" class="headerlink" title="3. replication slot"></a>3. replication slot</h1><p>replication slot 是postgresql 9.0 引入的一个特性(commit:<a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=858ec11858a914d4c380971985709b6d6b7dd6fc">https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=858ec11858a914d4c380971985709b6d6b7dd6fc</a>)，是crash-safe的数据结构，根据功能可分为physical和logical两种，根据生命周期又可分为persistent和ephemeral 两种。ephemeral的replication slot在系统restart或者release时，就会删除。physical replication slot是persistent类型的，logical replication slot是在创建时是ephemeral类型的，创建成功后调用ReplicationSlotPersist修改为persistent类型。</p><p>replication slot 有2种创建方式：</p><ol><li>系统内置函数：pg_create_logical_replication_slot/pg_create_physical_replication_slot、pg_drop_replication_slot</li><li>流复制命令：CREATE_REPLICATION_SLOT/DROP_REPLICATION_SLOT</li></ol><p>可以使用系统内置函数pg_get_replication_slots或者系统视图pg_replication_slots查询所有的replication slots。</p><h2 id="3-1-physical-replication-slot"><a href="#3-1-physical-replication-slot" class="headerlink" title="3.1 physical replication slot"></a>3.1 physical replication slot</h2><p>对于postgresql 而言physical replication slot可以解决流复制时遇到的两个常见问题：</p><ol><li>防止primary上的xlog 过早被回收</li></ol><p>在没有replication slot时，一般是通过wal_keep_segments参数，让primary 保留足够多的xlog，来解决这个问题。<br>physical replication slot解决该问题的方式是：wal sender 收到standby 发送的xlog flush location，然后把改xlog lsn更新到physical replication slot上。CreateCheckPoint在回收xlog时，会check physical replication slot的lsn，把最小的lsn之前的xlog回收掉。</p><ol start="2"><li>与hot_standby_feedback配合使用，防止primary 的vacuum过早的清理到hot standby需要查询的tuple</li></ol><p>wal sender收到wal receiver发送的xmin后，更新到replication slot， primary做vacuum时，会清理min(GetOldestXmin, min(all replication slot feedback xmin) 之前的所有无效版本。</p><p>使用physical replication slot需要注意一下一个问题：</p><ol><li>如果standby长时间未响应，primary replication slot lsn不会更新，会导致primary xlog堆积，有打满磁盘的风险。</li></ol><p>可以通过配置比较小的wal_sender_timeout来解决这个问题。</p><ol start="2"><li>如果hot standby有比较多的慢查询，肯会导致primary vacuum效果很差，无法回收无效的tuple 版本。</li></ol><p>对于greemplum6.0来说，目前physical replication slot的用处不大，因为：</p><ol><li>每个wal sender都会记录standby flush location，checkpoint回收xlog时会检查该lsn，防止过早回收xlog</li></ol><p>gp 加了一个xlogCleanUpTo，来避免xlog过早回收。但是需要注意gp会比较xlogCleanUpTo和replicationSlotMinLSN，选择最小的lsn。所以不要随便创建physical replication slot，不然可能会导致xlog堆积。</p><ol start="2"><li>不支持hot standby，也就不会有vacuum这个问题</li></ol><h2 id="3-2-logical-replication-slot"><a href="#3-2-logical-replication-slot" class="headerlink" title="3.2 logical replication slot"></a>3.2 logical replication slot</h2><p>logical replication slot是用来实现逻辑复制功能的，与physical replication slot的区别就在于physical replication slot 发送的是xlog record，而logical replication slot发送的是逻辑日志。该功能会在下一章的logical decoding详细分析。</p><h1 id="4-logical-decoding"><a href="#4-logical-decoding" class="headerlink" title="4. logical decoding"></a>4. logical decoding</h1><p>logical decoding是postgresql9.4 添加的一个新的功能，可以通过流的方法把数据库的修改通过sql的形式发送消费者。logical replication slot 标识这些修改，并且这些更改只会发送一次。<br>postgresql9.4 版本的logical decoding只支持insert/update/delete 操作，在后面postgresql11时支持了truncate 操作。<br>logical decoding在Greenplum6.0中也是支持的，不过无法对整个集群做，只能对每个segment、master做，可用性比较差。</p><h2 id="4-1-pg-recvlogical"><a href="#4-1-pg-recvlogical" class="headerlink" title="4.1 pg_recvlogical"></a>4.1 pg_recvlogical</h2><p>postgresql官方提供的一个用于接收服务端发送的logical log的工具，简单的用法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create one new logical replication slot for pg_recvlogical</span></span><br><span class="line">pg_recvlogical --create_slot -S test_slot -d postgres -p 12000</span><br><span class="line"></span><br><span class="line"><span class="comment"># start receive logical log</span></span><br><span class="line">pg_recvlogical --start -S test_slot -d postgres -p 12000 -f -</span><br></pre></td></tr></table></figure><p>pg_recvlogical的输出如下：<br><img src="/2020/12/28/Greenplum-6-0-HA/pg_recvlogical.png" alt="pg_recvlogical"><br>从上图可以看到delete的时候没有解析到删除的行，这个原因是xlog记录的不全导致的，可以通过alter table来修改表在update/delete时对于old tuple的处理方式：<br>DEFAULT：如果有primary key，则记录old tuple的primary key对应的column value， 该方式是user table的默认方式<br>USING INDEX indexname：记录old tuple中indexname key对应的column value<br>FULL：记录old tuple所有的column value<br>NOTHONG：不记录old tuple， system table 的默认方式</p><h2 id="4-2-logical-decoding实现"><a href="#4-2-logical-decoding实现" class="headerlink" title="4.2 logical decoding实现"></a>4.2 logical decoding实现</h2><p>logical decoding 的实现分为下面几个部分：</p><ol><li>logical decoding，负责把数据库的更改，转换成更容易理解的格式，在postgresql中是把wal日志解析成指定的格式。</li><li>replication slot，负责控制数据库change的发送，每个replication slot只能针对单一的数据库。</li><li>output plugin，负责把decode后更改，格式化，该功能已插件的形式提供，用户可以自己编写插件实现不同的输出形式，而不用修改任何数据库内核代码。postgresql官方提供了一个例子：test_decoding</li></ol><p>可以使用START_REPLICATION 指定使用的plugin</p><ol start="4"><li>exported snapshot，在创建logical replication slot时，会自动export一个snapshot，其他session可以import该snapshot，能够看到当时的数据库状态。</li></ol><p>该功能在postgresql9.5时，可以配合pg_dump –snapshot 功能，通过逻辑复制，通过整个database。</p><h2 id="4-3-相关系统函数"><a href="#4-3-相关系统函数" class="headerlink" title="4.3 相关系统函数"></a>4.3 相关系统函数</h2><ol><li>pg_logical_slot_get_changes</li><li>pg_logical_slot_peek_changes</li></ol><h1 id="5-相关tools"><a href="#5-相关tools" class="headerlink" title="5. 相关tools"></a>5. 相关tools</h1><p>postgresql自带的工具</p><ul><li><input checked disabled type="checkbox"> <strong>pg_recvlogical</strong></li></ul><p>用于接收服务器端发送的logical log，使用示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create one new logical replication slot for pg_recvlogical</span></span><br><span class="line">pg_recvlogical --create_slot -S test_slot -d postgres -p 12000</span><br><span class="line"></span><br><span class="line"><span class="comment"># start receive logical log</span></span><br><span class="line">pg_recvlogical --start -S test_slot -d postgres -p 12000 -f -</span><br></pre></td></tr></table></figure><ul><li><input checked disabled type="checkbox"> <strong>pg_receivexlog</strong></li></ul><p>与pg_recvlogical对应，用于接收服务器端发送的wal log</p><ul><li><input checked disabled type="checkbox"> <strong>pg_basebackup</strong></li></ul><p>用于对数据库创建base backup</p><ul><li><input checked disabled type="checkbox"> <strong>pg_rewind</strong></li></ul><p>用于修改old master data，方便把old master重新加入集群</p><p>greemplum添加的工具：</p><ul><li><input checked disabled type="checkbox"> <strong>gpinitstandby</strong></li></ul><p>为master添加standby节点，执行逻辑为：<br>Updating pg_hba.conf file<br>Do pg_basebackup<br>BASE_BACKUP LABEL ‘pg_basebackup base backup’ PROGRESS WAL FAST NOWAIT  EXCLUDE ‘./db_dumps’EXCLUDE ‘./gpperfmon/data’EXCLUDE ‘./gpperfmon/logs’EXCLUDE ‘./promote’<br>Starting standby master</p><ul><li><input checked disabled type="checkbox"> <strong>gpaddmirrors</strong></li></ul><p>为segment添加mirror节点，与gpinitstandby类似，都是对流复制协议的封装</p><ul><li><input checked disabled type="checkbox"> <strong>gprecoverseg</strong></li></ul><p>recovery 挂掉的segment，使用pg_rewind把旧节点重新加入到集群</p><h1 id="6-fts-fault-tolerance-server"><a href="#6-fts-fault-tolerance-server" class="headerlink" title="6. fts(fault tolerance server)"></a>6. fts(<strong>fault tolerance server)</strong></h1><p>master 监控primary和mirror状态，并在发生故障时，做主备切换。master只会给primary发探活消息，mirror的状态是primary通过wal sender状态来确定，并反馈给master的。</p><p>fts探活协议：<br>FTS_PROBE_SEGMENT<br>FTS_PROBE_SUCCESS<br>primary mirror alive and sync<br>mirror down<br>primary mirror had sync<br>FTS_PROBE_FAILED</p><p>primary:HandleFtsWalRepProbe</p><p>FTS_PROBE_SUCCESS<br>mirror down:FTS_SYNCREP_OFF_SEGMENT</p><p>FTS_SYNCREP_OFF_SEGMENT<br>primary: HandleFtsWalRepSyncRepOff</p><p>FTS_PROBE_FAILED<br>FTS_PROMOTE_SEGMENT</p><p>FTS_PROMOTE_SEGMENT<br>primary:HandleFtsWalRepPromote</p><p>FTS_SYNCREP_OFF_FAILED<br>FTS_PROMOTE_FAILED</p><p>fts相关的几个参数，以及线上默认值：<br>gp_fts_probe_retries：3<br>gp_fts_probe_interval: 30s<br>gp_request_fts_probe_scan<br>gp_fts_probe_timeout: 60s</p><p><strong>最大切换时间: 30 + 3 * (60 + 1) = 213s**<br>*<em>如果不发生timeout是: 30 + 3</em>1 = 33s</strong></p><h1 id="7-与Greenplum-4-3的对比"><a href="#7-与Greenplum-4-3的对比" class="headerlink" title="7. 与Greenplum 4.3的对比"></a>7. 与Greenplum 4.3的对比</h1><p><img src="/2020/12/28/Greenplum-6-0-HA/6.0-4.3-diff.png" alt="6.0与4.3对比"></p><p>Greenplum 4.3 的数据同步分为两种：<br>一是master与standby的同步，使用postgresql原生的streaming replication<br>二是primary与mirror的同步，使用greenplum自己开发的file replication</p><p>而Greenplum 6.0 的数据同步同一使用postgresql原生的streaming replication，这样做有比较多的好处：</p><ol><li>贴近postgresql 社区，后续内核代码合入更容易</li><li>streaming replication架构比replication更简单，而且streaming replication经过postgresql几年的发展更加成熟稳定</li><li>streaming replication比file replication更加的灵活，可以比较容易的实现全量备份+ 增量备份，而且后面还可以实现postgresql的PITR，多standby以及master的host standby等功能</li></ol><h1 id="8-参考资料"><a href="#8-参考资料" class="headerlink" title="8. 参考资料"></a>8. 参考资料</h1><ol><li><a href="http://hlinnaka.iki.fi/presentations/NordicPGDay2015-pg_rewind.pdf">http://hlinnaka.iki.fi/presentations/NordicPGDay2015-pg_rewind.pdf</a></li><li><a href="https://bbs.aliyun.com/read/244478.html">https://bbs.aliyun.com/read/244478.html</a></li><li><a href="https://www.postgresql.org/docs/9.4/protocol-replication.html">https://www.postgresql.org/docs/9.4/protocol-replication.html</a></li><li><a href="https://www.postgresql.org/docs/9.4/logicaldecoding.html">https://www.postgresql.org/docs/9.4/logicaldecoding.html</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-整体架构&quot;&gt;&lt;a href=&quot;#1-整体架构&quot; class=&quot;headerlink&quot; title=&quot;1. 整体架构&quot;&gt;&lt;/a&gt;1. 整体架构&lt;/h1&gt;&lt;p&gt;Greenplum 6.0 在HA的架构上与4.3基本上一样的，改动比较大的是primary/mirror之间的数据同步机制，由4.3的file replication 改为streaming replication。&lt;/p&gt;</summary>
    
    
    
    <category term="greenplum6.0" scheme="https://zisedeqing.github.io/categories/greenplum6-0/"/>
    
    <category term="ha" scheme="https://zisedeqing.github.io/categories/greenplum6-0/ha/"/>
    
    
    <category term="greenplum" scheme="https://zisedeqing.github.io/tags/greenplum/"/>
    
    <category term="streaming replication" scheme="https://zisedeqing.github.io/tags/streaming-replication/"/>
    
    <category term="mirror" scheme="https://zisedeqing.github.io/tags/mirror/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zisedeqing.github.io/2020/12/28/hello-world/"/>
    <id>https://zisedeqing.github.io/2020/12/28/hello-world/</id>
    <published>2020-12-28T08:59:53.742Z</published>
    <updated>2020-12-28T12:26:58.847Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
    <category term="test" scheme="https://zisedeqing.github.io/tags/test/"/>
    
  </entry>
  
</feed>
